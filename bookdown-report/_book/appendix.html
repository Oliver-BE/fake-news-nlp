<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Appendix | Classifying Fake News using NLP and ML</title>
  <meta name="description" content="An introductory look at using NLP and ML to classify news articles." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Appendix | Classifying Fake News using NLP and ML" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introductory look at using NLP and ML to classify news articles." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Appendix | Classifying Fake News using NLP and ML" />
  
  <meta name="twitter:description" content="An introductory look at using NLP and ML to classify news articles." />
  

<meta name="author" content="Oliver Baldwin Edwards" />


<meta name="date" content="2020-12-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="conclusion.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>2</b> Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="background.html"><a href="background.html#natural-language-processing"><i class="fa fa-check"></i><b>2.1</b> Natural Language Processing</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="background.html"><a href="background.html#reducing-a-vocabulary-with-lemmatization-and-stop-words"><i class="fa fa-check"></i><b>2.1.1</b> Reducing a Vocabulary with Lemmatization and Stop Words</a></li>
<li class="chapter" data-level="2.1.2" data-path="background.html"><a href="background.html#bag-of-words"><i class="fa fa-check"></i><b>2.1.2</b> Bag-of-Words Model for Feature Extraction</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="background.html"><a href="background.html#deep-learning-models"><i class="fa fa-check"></i><b>2.2</b> Deep Learning Models</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="background.html"><a href="background.html#multilayer-perceptrons"><i class="fa fa-check"></i><b>2.2.1</b> Multilayer Perceptrons</a></li>
<li class="chapter" data-level="2.2.2" data-path="background.html"><a href="background.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2.2</b> Recurrent Neural Networks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#data-cleaning"><i class="fa fa-check"></i><b>3.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>3.2</b> Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-fitting.html"><a href="model-fitting.html"><i class="fa fa-check"></i><b>4</b> Model Fitting</a></li>
<li class="chapter" data-level="5" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>5</b> Results</a></li>
<li class="chapter" data-level="6" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="conclusion.html"><a href="conclusion.html#limitations"><i class="fa fa-check"></i><b>6.1</b> Limitations</a></li>
<li class="chapter" data-level="6.2" data-path="conclusion.html"><a href="conclusion.html#future-work"><i class="fa fa-check"></i><b>6.2</b> Future Work</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>7</b> Appendix</a>
<ul>
<li class="chapter" data-level="7.1" data-path="appendix.html"><a href="appendix.html#appendix-data"><i class="fa fa-check"></i><b>7.1</b> Data Wrangling/Feature Extraction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="appendix.html"><a href="appendix.html#basic-data-wrangling"><i class="fa fa-check"></i><b>7.1.1</b> Basic Data Wrangling</a></li>
<li class="chapter" data-level="7.1.2" data-path="appendix.html"><a href="appendix.html#cleaning-the-textreducing-the-vocabulary-size"><i class="fa fa-check"></i><b>7.1.2</b> Cleaning the Text/Reducing the Vocabulary Size</a></li>
<li class="chapter" data-level="7.1.3" data-path="appendix.html"><a href="appendix.html#traintest-split"><i class="fa fa-check"></i><b>7.1.3</b> Train/Test Split</a></li>
<li class="chapter" data-level="7.1.4" data-path="appendix.html"><a href="appendix.html#feature-extraction"><i class="fa fa-check"></i><b>7.1.4</b> Creating a Document Term Matrix</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="appendix.html"><a href="appendix.html#appendix-initial-models"><i class="fa fa-check"></i><b>7.2</b> Initial Model Fitting</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="appendix.html"><a href="appendix.html#naive-bayes"><i class="fa fa-check"></i><b>7.2.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="7.2.2" data-path="appendix.html"><a href="appendix.html#basic-logistic-regression"><i class="fa fa-check"></i><b>7.2.2</b> Basic Logistic Regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="appendix.html"><a href="appendix.html#logistic-regresion-with-l1-penalty-lasso-regression"><i class="fa fa-check"></i><b>7.2.3</b> Logistic Regresion with L1 penalty (Lasso Regression)</a></li>
<li class="chapter" data-level="7.2.4" data-path="appendix.html"><a href="appendix.html#support-vector-machine"><i class="fa fa-check"></i><b>7.2.4</b> Support Vector Machine</a></li>
<li class="chapter" data-level="7.2.5" data-path="appendix.html"><a href="appendix.html#random-forest"><i class="fa fa-check"></i><b>7.2.5</b> Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="appendix.html"><a href="appendix.html#appendix-deeplearning-models"><i class="fa fa-check"></i><b>7.3</b> Deep Learning Model Fitting</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="appendix.html"><a href="appendix.html#multilayer-perceptron-neural-network"><i class="fa fa-check"></i><b>7.3.1</b> Multilayer Perceptron Neural Network</a></li>
<li class="chapter" data-level="7.3.2" data-path="appendix.html"><a href="appendix.html#recurrent-neural-network"><i class="fa fa-check"></i><b>7.3.2</b> Recurrent Neural Network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Classifying Fake News using NLP and ML</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendix" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Appendix</h1>
<p></p>
<p>The appendix walks through all of the code performed in this project including data wrangling, feature extraction, and model fitting.</p>
<div id="appendix-data" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Data Wrangling/Feature Extraction</h2>
<p></p>
<div id="basic-data-wrangling" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Basic Data Wrangling</h3>
<p>We begin by reading in our raw data:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="appendix.html#cb8-1" aria-hidden="true" tabindex="-1"></a>politifact_df_raw <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;../data-raw/politifact_phase2_clean_2018_7_3.csv&quot;</span>) </span></code></pre></div>
<p>We then clean up the raw data by getting rid of rows with duplicate URLs, getting rid of targets we don’t care about, and creating two columns for our targets (one numerical, one categorical) since different models require different variable types for target values. In addition, we create a unique ID for each row (which consists of an article’s claim and its PolitiFact truth rating).</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="appendix.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># clean up raw data</span></span>
<span id="cb9-2"><a href="appendix.html#cb9-2" aria-hidden="true" tabindex="-1"></a>politifact_df_cleaned <span class="ot">&lt;-</span> politifact_df_raw <span class="sc">%&gt;%</span> </span>
<span id="cb9-3"><a href="appendix.html#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get rid of duplicate URLs (reasoning for this is discussed in limitations)</span></span>
<span id="cb9-4"><a href="appendix.html#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>(politifact_url_phase1, <span class="at">.keep_all =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span>  </span>
<span id="cb9-5"><a href="appendix.html#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># keep only the two targets we care about </span></span>
<span id="cb9-6"><a href="appendix.html#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(fact_tag_phase1 <span class="sc">==</span> <span class="st">&quot;True&quot;</span> <span class="sc">|</span> fact_tag_phase1 <span class="sc">==</span> <span class="st">&quot;Pants on Fire!&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb9-7"><a href="appendix.html#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># change desired targets to categorical and numerical</span></span>
<span id="cb9-8"><a href="appendix.html#cb9-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">targets_numerical =</span> <span class="fu">as.numeric</span>(<span class="fu">ifelse</span>(fact_tag_phase1 <span class="sc">==</span> <span class="st">&quot;True&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)),</span>
<span id="cb9-9"><a href="appendix.html#cb9-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">targets_categorical =</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(fact_tag_phase1 <span class="sc">==</span> <span class="st">&quot;True&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb9-10"><a href="appendix.html#cb9-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># rename the article claim variable</span></span>
<span id="cb9-11"><a href="appendix.html#cb9-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">article_claim =</span> article_claim_phase1) <span class="sc">%&gt;%</span> </span>
<span id="cb9-12"><a href="appendix.html#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># only keep the variables we care about</span></span>
<span id="cb9-13"><a href="appendix.html#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(article_claim, targets_numerical, targets_categorical)</span>
<span id="cb9-14"><a href="appendix.html#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="appendix.html#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># add an id </span></span>
<span id="cb9-16"><a href="appendix.html#cb9-16" aria-hidden="true" tabindex="-1"></a>politifact_df_cleaned<span class="sc">$</span>article_id <span class="ot">&lt;-</span> <span class="fu">seq.int</span>(<span class="fu">nrow</span>(politifact_df_cleaned)) </span>
<span id="cb9-17"><a href="appendix.html#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># take a look at our cleaned dataframe</span></span>
<span id="cb9-18"><a href="appendix.html#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(politifact_df_cleaned)</span></code></pre></div>
<pre><code>## Rows: 1,911
## Columns: 4
## $ article_claim       &lt;chr&gt; &quot;\&quot;When you tally up their representation in Congress and governorships…
## $ targets_numerical   &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,…
## $ targets_categorical &lt;fct&gt; 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,…
## $ article_id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …</code></pre>
</div>
<div id="cleaning-the-textreducing-the-vocabulary-size" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Cleaning the Text/Reducing the Vocabulary Size</h3>
<p>Since the <code>article_claim</code> column contains messy text data, text cleaning is needed. The text cleaning done in this project consists of the following: removing punctuation, making all letters lowercase, removing English stop words, lemmatizing each word, removing numbers, and removing any extra white space. Removing stop words, numbers, and lemmatizing words all leads to a decrease in the size of the overall vocabulary (all unique words across a set of text).</p>
<p>The <code>clean_text</code> function performs the aforementioned text cleaning and vocabulary reduction:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="appendix.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to perform text cleaning</span></span>
<span id="cb11-2"><a href="appendix.html#cb11-2" aria-hidden="true" tabindex="-1"></a>clean_text <span class="ot">&lt;-</span> <span class="cf">function</span>(input_text) {</span>
<span id="cb11-3"><a href="appendix.html#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove punctuation</span></span>
<span id="cb11-4"><a href="appendix.html#cb11-4" aria-hidden="true" tabindex="-1"></a>  output_text <span class="ot">&lt;-</span> <span class="fu">removePunctuation</span>(input_text) <span class="sc">%&gt;%</span> </span>
<span id="cb11-5"><a href="appendix.html#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make all letters lowercase</span></span>
<span id="cb11-6"><a href="appendix.html#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tolower</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb11-7"><a href="appendix.html#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove a custom list of English stop words (from the `tm` package)</span></span>
<span id="cb11-8"><a href="appendix.html#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">removeWords</span>(<span class="fu">stopwords</span>(<span class="st">&quot;en&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb11-9"><a href="appendix.html#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># lemmatize the words in the text </span></span>
<span id="cb11-10"><a href="appendix.html#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lemmatize_strings</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb11-11"><a href="appendix.html#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove any numbers in the text</span></span>
<span id="cb11-12"><a href="appendix.html#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">removeNumbers</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb11-13"><a href="appendix.html#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get rid of any extra white space</span></span>
<span id="cb11-14"><a href="appendix.html#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stripWhitespace</span>()</span>
<span id="cb11-15"><a href="appendix.html#cb11-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-16"><a href="appendix.html#cb11-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output_text)</span>
<span id="cb11-17"><a href="appendix.html#cb11-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-18"><a href="appendix.html#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="appendix.html#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># an example of using the `clean_text` function</span></span>
<span id="cb11-20"><a href="appendix.html#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="fu">clean_text</span>(<span class="st">&quot;&#39;Trump approval rating    better than Obama  and Reagan at </span></span>
<span id="cb11-21"><a href="appendix.html#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="st">            same point in their presidencies.&#39;&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;trump approval rate good obama reagan point presidency&quot;</code></pre>
<p>The <code>clean_text</code> function is used to clean up the text in <code>politifact_df_cleaned</code>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="appendix.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># clean up the article claim text, create a final dataframe</span></span>
<span id="cb13-2"><a href="appendix.html#cb13-2" aria-hidden="true" tabindex="-1"></a>politifact_df_final <span class="ot">&lt;-</span> politifact_df_cleaned <span class="sc">%&gt;%</span> </span>
<span id="cb13-3"><a href="appendix.html#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">article_text =</span> <span class="fu">clean_text</span>(article_claim)) <span class="sc">%&gt;%</span> </span>
<span id="cb13-4"><a href="appendix.html#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(article_id, article_text, targets_numerical, targets_categorical)</span>
<span id="cb13-5"><a href="appendix.html#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="appendix.html#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(politifact_df_final) </span></code></pre></div>
<pre><code>## Rows: 1,911
## Columns: 4
## $ article_id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …
## $ article_text        &lt;chr&gt; &quot;tally representation congress governorship democrat almost low represe…
## $ targets_numerical   &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,…
## $ targets_categorical &lt;fct&gt; 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,…</code></pre>
</div>
<div id="traintest-split" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Train/Test Split</h3>
<p>In order to evaluate classification models, a training set and a testing set are needed. In this project, an <span class="math inline">\(80%\)</span>/<span class="math inline">\(20%\)</span> training/testing split is used.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="appendix.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set the seed to ensure reproducibility</span></span>
<span id="cb15-2"><a href="appendix.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb15-3"><a href="appendix.html#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="appendix.html#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the number of rows in our final data frame </span></span>
<span id="cb15-5"><a href="appendix.html#cb15-5" aria-hidden="true" tabindex="-1"></a>num_politifact_rows <span class="ot">&lt;-</span> <span class="fu">nrow</span>(politifact_df_final)</span>
<span id="cb15-6"><a href="appendix.html#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="appendix.html#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># get random row numbers from our data frame (80% of all possible row numbers</span></span>
<span id="cb15-8"><a href="appendix.html#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># will be stored in `random_row_numbers` since we have an 80/20 split)</span></span>
<span id="cb15-9"><a href="appendix.html#cb15-9" aria-hidden="true" tabindex="-1"></a>random_row_numbers <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>num_politifact_rows, <span class="fl">0.8</span> <span class="sc">*</span> num_politifact_rows)</span>
<span id="cb15-10"><a href="appendix.html#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="appendix.html#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># use our randomly generated row numbers to select 80% of our data for training</span></span>
<span id="cb15-12"><a href="appendix.html#cb15-12" aria-hidden="true" tabindex="-1"></a>politifact_df_train  <span class="ot">&lt;-</span> politifact_df_final[random_row_numbers, ] <span class="sc">%&gt;%</span> </span>
<span id="cb15-13"><a href="appendix.html#cb15-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># sort by article id</span></span>
<span id="cb15-14"><a href="appendix.html#cb15-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(article_id)</span>
<span id="cb15-15"><a href="appendix.html#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># and use the remaining 20% of our data for testing</span></span>
<span id="cb15-16"><a href="appendix.html#cb15-16" aria-hidden="true" tabindex="-1"></a>politifact_df_test   <span class="ot">&lt;-</span> politifact_df_final[<span class="sc">-</span>random_row_numbers, ] <span class="sc">%&gt;%</span> </span>
<span id="cb15-17"><a href="appendix.html#cb15-17" aria-hidden="true" tabindex="-1"></a>   <span class="fu">arrange</span>(article_id)</span>
<span id="cb15-18"><a href="appendix.html#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="appendix.html#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># keep track of our target variables in a list </span></span>
<span id="cb15-20"><a href="appendix.html#cb15-20" aria-hidden="true" tabindex="-1"></a>train_targets_numerical <span class="ot">&lt;-</span> politifact_df_train<span class="sc">$</span>targets_numerical</span>
<span id="cb15-21"><a href="appendix.html#cb15-21" aria-hidden="true" tabindex="-1"></a>train_targets_categorical <span class="ot">&lt;-</span> politifact_df_train<span class="sc">$</span>targets_categorical</span>
<span id="cb15-22"><a href="appendix.html#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="appendix.html#cb15-23" aria-hidden="true" tabindex="-1"></a>test_targets_numerical <span class="ot">&lt;-</span> politifact_df_test<span class="sc">$</span>targets_numerical</span>
<span id="cb15-24"><a href="appendix.html#cb15-24" aria-hidden="true" tabindex="-1"></a>test_targets_categorical <span class="ot">&lt;-</span> politifact_df_test<span class="sc">$</span>targets_categorical</span></code></pre></div>
<p>We next want to make sure our training/testing split has a roughly equal distribution of each target value.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="appendix.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(train_targets_numerical))</span></code></pre></div>
<pre><code>## train_targets_numerical
##         0         1 
## 0.4443717 0.5556283</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="appendix.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(test_targets_numerical))</span></code></pre></div>
<pre><code>## test_targets_numerical
##         0         1 
## 0.4830287 0.5169713</code></pre>
<p>Across our two target variables—where a “false” piece of news corresponds to <span class="math inline">\(0\)</span> and a “true” piece of news corresponds to <span class="math inline">\(1\)</span>—we observe a <span class="math inline">\(44.4%\)</span>, <span class="math inline">\(55.6%\)</span> to <span class="math inline">\(48.3%\)</span>, <span class="math inline">\(51.7%\)</span> training/testing split. Since the proportion of target values is reasonably even, we can proceed with this split.</p>
</div>
<div id="feature-extraction" class="section level3" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Creating a Document Term Matrix</h3>
<p></p>
<p>To create a Document Term Matrix (DTM), the <code>article_text</code> needs to be tokenized into individual words so that an overall vocabulary can be created. Here n-grams are used (from <span class="math inline">\(n=1\)</span> to <span class="math inline">\(n=3\)</span>). This means that all possible <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, and <span class="math inline">\(3\)</span> word combinations are included in the vocabulary. Note that we must create our vocabulary using only the training data, since our test data should not be looked at until model evaluation.</p>
<p>Once a vocabulary is created, terms that only appeared once throughout our training dataset are removed. This reduces the size of the vocabulary terms from <span class="math inline">\(28678\)</span> terms to <span class="math inline">\(806\)</span>, which is a far more manageable number of terms to have in a DTM.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="appendix.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tokenize the text into individual words</span></span>
<span id="cb20-2"><a href="appendix.html#cb20-2" aria-hidden="true" tabindex="-1"></a>tokenizer_train <span class="ot">&lt;-</span> <span class="fu">word_tokenizer</span>(politifact_df_train<span class="sc">$</span>article_text) <span class="sc">%&gt;%</span> </span>
<span id="cb20-3"><a href="appendix.html#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">itoken</span>(<span class="at">ids =</span> politifact_df_train<span class="sc">$</span>article_id, <span class="at">progressbar =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-4"><a href="appendix.html#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="appendix.html#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># this test tokenizer will be used later</span></span>
<span id="cb20-6"><a href="appendix.html#cb20-6" aria-hidden="true" tabindex="-1"></a>tokenizer_test <span class="ot">&lt;-</span> <span class="fu">word_tokenizer</span>(politifact_df_test<span class="sc">$</span>article_text) <span class="sc">%&gt;%</span> </span>
<span id="cb20-7"><a href="appendix.html#cb20-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">itoken</span>(<span class="at">ids =</span> politifact_df_test<span class="sc">$</span>article_id, <span class="at">progressbar =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-8"><a href="appendix.html#cb20-8" aria-hidden="true" tabindex="-1"></a>         </span>
<span id="cb20-9"><a href="appendix.html#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># create our vocabulary with the training data</span></span>
<span id="cb20-10"><a href="appendix.html#cb20-10" aria-hidden="true" tabindex="-1"></a>vocabulary <span class="ot">&lt;-</span> <span class="fu">create_vocabulary</span>(tokenizer_train, <span class="at">ngram =</span> <span class="fu">c</span>(1L, 3L))</span>
<span id="cb20-11"><a href="appendix.html#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># we observe 28678 unique terms, this is too large</span></span>
<span id="cb20-12"><a href="appendix.html#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(vocabulary)</span></code></pre></div>
<pre><code>## [1] 28678</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="appendix.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prune our vocabulary to only include terms used at least 2 times</span></span>
<span id="cb22-2"><a href="appendix.html#cb22-2" aria-hidden="true" tabindex="-1"></a>pruned_vocabulary <span class="ot">&lt;-</span> <span class="fu">prune_vocabulary</span>(vocabulary, <span class="at">term_count_min =</span> <span class="dv">5</span>)</span>
<span id="cb22-3"><a href="appendix.html#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># we now observe 806 unique terms, which is more manageable</span></span>
<span id="cb22-4"><a href="appendix.html#cb22-4" aria-hidden="true" tabindex="-1"></a>pruned_vocabulary</span></code></pre></div>
<pre><code>## Number of docs: 1528 
## 0 stopwords:  ... 
## ngram_min = 1; ngram_max = 3 
## Vocabulary: 
##                 term term_count doc_count
##   1:         account          5         5
##   2:          affair          5         5
##   3:          affect          5         4
##   4: africanamerican          5         5
##   5:           agree          5         5
##  ---                                     
## 802:         percent        130       102
## 803:           obama        131       130
## 804:            year        149       130
## 805:           state        171       159
## 806:             say        417       372</code></pre>
<p>Now that we have a reasonably sized vocabulary, we can create our training and testing DTMs.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="appendix.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create our training and testing DTMs using our tokenizers</span></span>
<span id="cb24-2"><a href="appendix.html#cb24-2" aria-hidden="true" tabindex="-1"></a>vocabulary_vectorizer <span class="ot">&lt;-</span> <span class="fu">vocab_vectorizer</span>(pruned_vocabulary) </span>
<span id="cb24-3"><a href="appendix.html#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="appendix.html#cb24-4" aria-hidden="true" tabindex="-1"></a>dtm_train <span class="ot">&lt;-</span> <span class="fu">create_dtm</span>(tokenizer_train, vocabulary_vectorizer) </span>
<span id="cb24-5"><a href="appendix.html#cb24-5" aria-hidden="true" tabindex="-1"></a>dtm_test <span class="ot">&lt;-</span> <span class="fu">create_dtm</span>(tokenizer_test, vocabulary_vectorizer)  </span></code></pre></div>
<p>We then change our DTMs to hold term frequency–inverse document frequency (tf-idf) values, which will normalize the DTMs and increase the weight of terms which are specific to a single document (or handful of documents) and decrease the weight for terms used in many documents.<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="appendix.html#cb25-1" aria-hidden="true" tabindex="-1"></a>tfidf <span class="ot">&lt;-</span> TfIdf<span class="sc">$</span><span class="fu">new</span>()</span>
<span id="cb25-2"><a href="appendix.html#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="appendix.html#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model to train data and transform train data with fitted model</span></span>
<span id="cb25-4"><a href="appendix.html#cb25-4" aria-hidden="true" tabindex="-1"></a>dtm_train <span class="ot">&lt;-</span>  <span class="fu">fit_transform</span>(dtm_train, tfidf)</span>
<span id="cb25-5"><a href="appendix.html#cb25-5" aria-hidden="true" tabindex="-1"></a>dtm_test <span class="ot">&lt;-</span>  <span class="fu">fit_transform</span>(dtm_test, tfidf)</span>
<span id="cb25-6"><a href="appendix.html#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="appendix.html#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># convert our DTMs to a matrix and data frame--different formats are needed</span></span>
<span id="cb25-8"><a href="appendix.html#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># for different models (`keras` only takes matrices for example)</span></span>
<span id="cb25-9"><a href="appendix.html#cb25-9" aria-hidden="true" tabindex="-1"></a>dtm_train_matrix <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dtm_train)</span>
<span id="cb25-10"><a href="appendix.html#cb25-10" aria-hidden="true" tabindex="-1"></a>dtm_test_matrix <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dtm_test)</span>
<span id="cb25-11"><a href="appendix.html#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="appendix.html#cb25-12" aria-hidden="true" tabindex="-1"></a>dtm_train_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(dtm_train_matrix)</span>
<span id="cb25-13"><a href="appendix.html#cb25-13" aria-hidden="true" tabindex="-1"></a>dtm_test_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(dtm_test_matrix)</span></code></pre></div>
<p>The data wrangling and feature extraction is now complete. Our features exist in the form of Document Term Matrices where each column represents a term, each row represents a PolitiFact article, and each cell holds the term frequency–inverse document frequency value of that term. Our targets exist as <span class="math inline">\(0\)</span>’s and <span class="math inline">\(1\)</span>’s, where a “false” piece of news corresponds to 0 and a “true” piece of news corresponds to 1. We observe a small portion of our training and testing DTMs below. Note that each value is <span class="math inline">\(0\)</span> since our vocabulary contains <span class="math inline">\(3000\)</span> terms and it is very unlikely that the first <span class="math inline">\(5\)</span> terms are in any of the <span class="math inline">\(10\)</span> different rows we are looking at.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="appendix.html#cb26-1" aria-hidden="true" tabindex="-1"></a>dtm_train_df[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code></pre></div>
<pre><code>##   account affair affect africanamerican agree
## 1       0      0      0               0     0
## 3       0      0      0               0     0
## 6       0      0      0               0     0
## 8       0      0      0               0     0
## 9       0      0      0               0     0</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="appendix.html#cb28-1" aria-hidden="true" tabindex="-1"></a>dtm_test_df[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code></pre></div>
<pre><code>##    account affair affect africanamerican agree
## 2        0      0      0               0     0
## 4        0      0      0               0     0
## 5        0      0      0               0     0
## 7        0      0      0               0     0
## 12       0      0      0               0     0</code></pre>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="appendix-initial-models" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Initial Model Fitting</h2>
<p></p>
<p>This section displays the code for each of the initial models fit in an attempt to classify fake news.</p>
<div id="naive-bayes" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Naive Bayes</h3>
<p>We can use the <code>e1071</code> package to run a Naive Bayes model. Note that the <code>e1071::naiveBayes</code> function requires categorical targets and accepts features in the form of a dataframe.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="appendix.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># applying Naive Bayes model to training data</span></span>
<span id="cb30-2"><a href="appendix.html#cb30-2" aria-hidden="true" tabindex="-1"></a>nb_model <span class="ot">&lt;-</span> e1071<span class="sc">::</span><span class="fu">naiveBayes</span>(<span class="at">x =</span> dtm_train_df, <span class="at">y =</span> train_targets_categorical, </span>
<span id="cb30-3"><a href="appendix.html#cb30-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">laplace =</span> <span class="dv">1</span>)</span>
<span id="cb30-4"><a href="appendix.html#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="appendix.html#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># predicting applying to test set</span></span>
<span id="cb30-6"><a href="appendix.html#cb30-6" aria-hidden="true" tabindex="-1"></a>nb_predicted_values <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb_model, dtm_test_df)</span>
<span id="cb30-7"><a href="appendix.html#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="appendix.html#cb30-8" aria-hidden="true" tabindex="-1"></a>nb_confusion_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(nb_predicted_values,</span>
<span id="cb30-9"><a href="appendix.html#cb30-9" aria-hidden="true" tabindex="-1"></a>                                              test_targets_categorical,</span>
<span id="cb30-10"><a href="appendix.html#cb30-10" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>)) </span>
<span id="cb30-11"><a href="appendix.html#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix table</span></span>
<span id="cb30-12"><a href="appendix.html#cb30-12" aria-hidden="true" tabindex="-1"></a>nb_confusion_matrix<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 137 104
##         1  48  94</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="appendix.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 60.3% accuracy</span></span>
<span id="cb32-2"><a href="appendix.html#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(nb_confusion_matrix<span class="sc">$</span>overall[<span class="dv">1</span>], <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Accuracy 
##    0.603</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="appendix.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [55.2%, 65.2%] 95% CI for accuracy</span></span>
<span id="cb34-2"><a href="appendix.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(nb_confusion_matrix<span class="sc">$</span>overall[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## AccuracyLower AccuracyUpper 
##         0.552         0.652</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="appendix.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for full information, uncomment and run the following line of code:</span></span>
<span id="cb36-2"><a href="appendix.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># nb_confusion_matrix</span></span></code></pre></div>
<p>Our Naive Bayes model performs rather poorly with an accuracy of only <span class="math inline">\(60.3\%\)</span> (and a 95% confidence interval of <span class="math inline">\([55.2\%, 65.2\%]\)</span>).</p>
</div>
<div id="basic-logistic-regression" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Basic Logistic Regression</h3>
<p>Since the <code>glm</code> function requires a formula, we must create a new dataframe combining our features and targets. Note that the <code>glm</code> function requires categorical targets.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="appendix.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic regression data frames (training and testing)</span></span>
<span id="cb37-2"><a href="appendix.html#cb37-2" aria-hidden="true" tabindex="-1"></a>lr_df_train <span class="ot">&lt;-</span> <span class="fu">cbind</span>(dtm_train_df, train_targets_categorical)</span>
<span id="cb37-3"><a href="appendix.html#cb37-3" aria-hidden="true" tabindex="-1"></a>lr_df_test <span class="ot">&lt;-</span> <span class="fu">cbind</span>(dtm_test_df, test_targets_categorical)</span></code></pre></div>
<p>We now fit the logisitic regression model and calculate its accuracy.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="appendix.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the basic logistic regression model</span></span>
<span id="cb38-2"><a href="appendix.html#cb38-2" aria-hidden="true" tabindex="-1"></a>blr_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(train_targets_categorical <span class="sc">~</span> ., <span class="at">data =</span> lr_df_train,</span>
<span id="cb38-3"><a href="appendix.html#cb38-3" aria-hidden="true" tabindex="-1"></a>                           <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb38-4"><a href="appendix.html#cb38-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb38-5"><a href="appendix.html#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># calculated the predicted value probabilities (which gives us </span></span>
<span id="cb38-6"><a href="appendix.html#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co"># the probability that an observation is classified as 1)</span></span>
<span id="cb38-7"><a href="appendix.html#cb38-7" aria-hidden="true" tabindex="-1"></a>blr_predicted_value_probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(blr_model, <span class="at">newdata =</span> lr_df_test, </span>
<span id="cb38-8"><a href="appendix.html#cb38-8" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb38-9"><a href="appendix.html#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="appendix.html#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co"># turn the probabilites into 1&#39;s or 0&#39;s--if the probability of being a 1</span></span>
<span id="cb38-11"><a href="appendix.html#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># is greater than 50%, turn it into a 1 (turn it into a 0 otherwise)</span></span>
<span id="cb38-12"><a href="appendix.html#cb38-12" aria-hidden="true" tabindex="-1"></a>blr_predicted_values <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(blr_predicted_value_probs <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb38-13"><a href="appendix.html#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="appendix.html#cb38-14" aria-hidden="true" tabindex="-1"></a>blr_confusion_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(blr_predicted_values,</span>
<span id="cb38-15"><a href="appendix.html#cb38-15" aria-hidden="true" tabindex="-1"></a>                                               test_targets_categorical,</span>
<span id="cb38-16"><a href="appendix.html#cb38-16" aria-hidden="true" tabindex="-1"></a>                                               <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>))</span>
<span id="cb38-17"><a href="appendix.html#cb38-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-18"><a href="appendix.html#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix table</span></span>
<span id="cb38-19"><a href="appendix.html#cb38-19" aria-hidden="true" tabindex="-1"></a>blr_confusion_matrix<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 102  63
##         1  83 135</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="appendix.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 61.9% accuracy</span></span>
<span id="cb40-2"><a href="appendix.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(blr_confusion_matrix<span class="sc">$</span>overall[<span class="dv">1</span>], <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Accuracy 
##    0.619</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="appendix.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [56.8%, 66.8%] 95% CI for accuracy</span></span>
<span id="cb42-2"><a href="appendix.html#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(blr_confusion_matrix<span class="sc">$</span>overall[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">3</span>) </span></code></pre></div>
<pre><code>## AccuracyLower AccuracyUpper 
##         0.568         0.668</code></pre>
<p>Our basic logistic regression model performs rather poorly with an accuracy of only <span class="math inline">\(61.9\%\)</span> (and a 95% confidence interval of <span class="math inline">\([56.8\%, 66.8\%]\)</span>).</p>
</div>
<div id="logistic-regresion-with-l1-penalty-lasso-regression" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Logistic Regresion with L1 penalty (Lasso Regression)</h3>
<p>This model is an improved form of logistic regression using an L1 regularization penalty and 5-fold cross-validation using the <code>glmnet</code> package. Logistic regression using an L1 regularization is also known as “Lasso regression.” Lasso regression shrinks the less important features’ coefficients to zero, thus removing some features altogether.<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a> This works well for feature selection when there are a large number of features, as in our case (we have <span class="math inline">\(806\)</span> features).</p>
<p>N-fold cross-validation (here <span class="math inline">\(n=5\)</span>) is used to “flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset.”<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a></p>
<p>The <code>cv.glmnet</code> function from the <code>glmnet</code> function is used here to perform the Lasso regression and 5-fold cross-validation discussed above. Note that our features must be in the form of a matrix here (not a dataframe) and our targets must be numerical.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="appendix.html#cb44-1" aria-hidden="true" tabindex="-1"></a>num_folds <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb44-2"><a href="appendix.html#cb44-2" aria-hidden="true" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb44-3"><a href="appendix.html#cb44-3" aria-hidden="true" tabindex="-1"></a>max_num_it <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb44-4"><a href="appendix.html#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="appendix.html#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co"># manually compute our folds so below model doesn&#39;t vary each time it&#39;s run</span></span>
<span id="cb44-6"><a href="appendix.html#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb44-7"><a href="appendix.html#cb44-7" aria-hidden="true" tabindex="-1"></a>fold_id <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="fu">seq</span>(num_folds), <span class="at">length.out =</span> <span class="fu">nrow</span>(dtm_train_matrix)))</span>
<span id="cb44-8"><a href="appendix.html#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="appendix.html#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="co"># use cross validation to determine the optimal lambda value for L1 penalty</span></span>
<span id="cb44-10"><a href="appendix.html#cb44-10" aria-hidden="true" tabindex="-1"></a>lasso_model <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> dtm_train_matrix, <span class="at">y =</span> train_targets_numerical, </span>
<span id="cb44-11"><a href="appendix.html#cb44-11" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># this gives us logistic regression</span></span>
<span id="cb44-12"><a href="appendix.html#cb44-12" aria-hidden="true" tabindex="-1"></a>                              <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>, </span>
<span id="cb44-13"><a href="appendix.html#cb44-13" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># L1 penalty</span></span>
<span id="cb44-14"><a href="appendix.html#cb44-14" aria-hidden="true" tabindex="-1"></a>                              <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb44-15"><a href="appendix.html#cb44-15" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># interested in the area under the ROC curve</span></span>
<span id="cb44-16"><a href="appendix.html#cb44-16" aria-hidden="true" tabindex="-1"></a>                              <span class="at">type.measure =</span> <span class="st">&quot;auc&quot;</span>,</span>
<span id="cb44-17"><a href="appendix.html#cb44-17" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># 5-fold cross-validation</span></span>
<span id="cb44-18"><a href="appendix.html#cb44-18" aria-hidden="true" tabindex="-1"></a>                              <span class="at">nfolds =</span> num_folds,</span>
<span id="cb44-19"><a href="appendix.html#cb44-19" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># manually create our folds for reproducibility</span></span>
<span id="cb44-20"><a href="appendix.html#cb44-20" aria-hidden="true" tabindex="-1"></a>                              <span class="at">foldid =</span> fold_id,</span>
<span id="cb44-21"><a href="appendix.html#cb44-21" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># a higher threshold is less accurate, </span></span>
<span id="cb44-22"><a href="appendix.html#cb44-22" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># but has faster training</span></span>
<span id="cb44-23"><a href="appendix.html#cb44-23" aria-hidden="true" tabindex="-1"></a>                              <span class="at">thresh =</span> threshold,</span>
<span id="cb44-24"><a href="appendix.html#cb44-24" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># a lower number of iterations is less accurate,</span></span>
<span id="cb44-25"><a href="appendix.html#cb44-25" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># but has faster training</span></span>
<span id="cb44-26"><a href="appendix.html#cb44-26" aria-hidden="true" tabindex="-1"></a>                              <span class="at">maxit =</span> max_num_it) </span>
<span id="cb44-27"><a href="appendix.html#cb44-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-28"><a href="appendix.html#cb44-28" aria-hidden="true" tabindex="-1"></a>lasso_predicted_value_probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_model, dtm_test_matrix, </span>
<span id="cb44-29"><a href="appendix.html#cb44-29" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb44-30"><a href="appendix.html#cb44-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-31"><a href="appendix.html#cb44-31" aria-hidden="true" tabindex="-1"></a>lasso_predicted_values <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(lasso_predicted_value_probs <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb44-32"><a href="appendix.html#cb44-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-33"><a href="appendix.html#cb44-33" aria-hidden="true" tabindex="-1"></a>lasso_confusion_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(lasso_predicted_values,</span>
<span id="cb44-34"><a href="appendix.html#cb44-34" aria-hidden="true" tabindex="-1"></a>                                                 test_targets_categorical,</span>
<span id="cb44-35"><a href="appendix.html#cb44-35" aria-hidden="true" tabindex="-1"></a>                                                 <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>))</span>
<span id="cb44-36"><a href="appendix.html#cb44-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-37"><a href="appendix.html#cb44-37" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix table</span></span>
<span id="cb44-38"><a href="appendix.html#cb44-38" aria-hidden="true" tabindex="-1"></a>lasso_confusion_matrix<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0  91  22
##         1  94 176</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="appendix.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 69.7% accuracy</span></span>
<span id="cb46-2"><a href="appendix.html#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(lasso_confusion_matrix<span class="sc">$</span>overall[<span class="dv">1</span>], <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Accuracy 
##    0.697</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="appendix.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [64.8%, 74.3%] 95% CI for accuracy</span></span>
<span id="cb48-2"><a href="appendix.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(lasso_confusion_matrix<span class="sc">$</span>overall[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">3</span>) </span></code></pre></div>
<pre><code>## AccuracyLower AccuracyUpper 
##         0.648         0.743</code></pre>
<p>Our logistic regression model using an L1 regularization penalty performs significantly better than our basic logistic regression model. This logistic regression model has an accuracy of <span class="math inline">\(69.7\%\)</span> (and a 95% confidence interval of <span class="math inline">\([64.8\%, 74.3\%]\)</span>).</p>
</div>
<div id="support-vector-machine" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Support Vector Machine</h3>
<p>We can use the <code>e1071</code> package to run a Support Vector Machine (SVM). Note that the <code>randomForest</code> function requires categorical targets and accepts features in the form of a dataframe.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="appendix.html#cb50-1" aria-hidden="true" tabindex="-1"></a>svm_model <span class="ot">&lt;-</span> e1071<span class="sc">::</span><span class="fu">svm</span>(<span class="at">x =</span> dtm_train_df, <span class="at">y =</span> train_targets_categorical, </span>
<span id="cb50-2"><a href="appendix.html#cb50-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>, <span class="at">kernel =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb50-3"><a href="appendix.html#cb50-3" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb50-4"><a href="appendix.html#cb50-4" aria-hidden="true" tabindex="-1"></a>svm_predicted_values <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_model, dtm_test_df)</span>
<span id="cb50-5"><a href="appendix.html#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="appendix.html#cb50-6" aria-hidden="true" tabindex="-1"></a>svm_confusion_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(svm_predicted_values,</span>
<span id="cb50-7"><a href="appendix.html#cb50-7" aria-hidden="true" tabindex="-1"></a>                                               test_targets_categorical,</span>
<span id="cb50-8"><a href="appendix.html#cb50-8" aria-hidden="true" tabindex="-1"></a>                                               <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>))</span>
<span id="cb50-9"><a href="appendix.html#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="appendix.html#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix table</span></span>
<span id="cb50-11"><a href="appendix.html#cb50-11" aria-hidden="true" tabindex="-1"></a>svm_confusion_matrix<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 113  46
##         1  72 152</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="appendix.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 69.2% accuracy</span></span>
<span id="cb52-2"><a href="appendix.html#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(svm_confusion_matrix<span class="sc">$</span>overall[<span class="dv">1</span>], <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Accuracy 
##    0.692</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="appendix.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [64.3%, 73.8%] 95% CI for accuracy</span></span>
<span id="cb54-2"><a href="appendix.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(svm_confusion_matrix<span class="sc">$</span>overall[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">3</span>) </span></code></pre></div>
<pre><code>## AccuracyLower AccuracyUpper 
##         0.643         0.738</code></pre>
<p>Our SVM performs reasonably well with an accuracy of <span class="math inline">\(69.2\%\)</span> (and a 95% confidence interval of <span class="math inline">\([64.3\%, 73.8\%]\)</span>).</p>
</div>
<div id="random-forest" class="section level3" number="7.2.5">
<h3><span class="header-section-number">7.2.5</span> Random Forest</h3>
<p>We can use the <code>randomForest</code> package to run a Naive Bayes model. Note that the <code>randomForest</code> function requires categorical targets and accepts features in the form of a dataframe.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="appendix.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb56-2"><a href="appendix.html#cb56-2" aria-hidden="true" tabindex="-1"></a>rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="at">x =</span> dtm_train_df, <span class="at">y =</span> train_targets_categorical)</span>
<span id="cb56-3"><a href="appendix.html#cb56-3" aria-hidden="true" tabindex="-1"></a>rf_predicted_values <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf_model, dtm_test_df)</span>
<span id="cb56-4"><a href="appendix.html#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="appendix.html#cb56-5" aria-hidden="true" tabindex="-1"></a>rf_confusion_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(rf_predicted_values,</span>
<span id="cb56-6"><a href="appendix.html#cb56-6" aria-hidden="true" tabindex="-1"></a>                                              test_targets_categorical,</span>
<span id="cb56-7"><a href="appendix.html#cb56-7" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>))</span>
<span id="cb56-8"><a href="appendix.html#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="appendix.html#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix table</span></span>
<span id="cb56-10"><a href="appendix.html#cb56-10" aria-hidden="true" tabindex="-1"></a>rf_confusion_matrix<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 124  51
##         1  61 147</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="appendix.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 70.8% accuracy</span></span>
<span id="cb58-2"><a href="appendix.html#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rf_confusion_matrix<span class="sc">$</span>overall[<span class="dv">1</span>], <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Accuracy 
##    0.708</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="appendix.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [65.9%, 75.3%] 95% CI for accuracy</span></span>
<span id="cb60-2"><a href="appendix.html#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rf_confusion_matrix<span class="sc">$</span>overall[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">3</span>) </span></code></pre></div>
<pre><code>## AccuracyLower AccuracyUpper 
##         0.659         0.753</code></pre>
<p>Our random forest model performs reasonably well with an accuracy of <span class="math inline">\(70.8\%\)</span> (and a 95% confidence interval of <span class="math inline">\([65.9\%, 75.3\%]\)</span>).</p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="appendix-deeplearning-models" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Deep Learning Model Fitting</h2>
<p></p>
<p>This section displays the code for the deep learning models fit in an attempt to classify fake news.</p>
<div id="multilayer-perceptron-neural-network" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Multilayer Perceptron Neural Network</h3>
<p>The <code>keras</code> package is used to fit a Multilayer Perceptron (MLP). Here, a MLP with one input layer, one hidden layer, and one output layer is created. The <code>keras</code> requires features to be stored in matrices and targets to be numerical.</p>
<p>Note that the <code>keras</code> package randomizes initial weights each time a model is run, meaning that results will vary slightly each time the following two code chunks are run (specifically, Figure <a href="appendix.html#fig:mlp-history">7.1</a> will change and the MLP model’s accuracy will change). The provided function to ensure reproducibility in <code>keras</code>(which is named <code>use_session_with_seed()</code>) does not currently work within a knitted R Markdown file. Additionally, using the normal <code>set.seed()</code> function does not alleviate the issue. For a more detailed breakdown, I’ve created a file outlining the problem that can be found in the directory <code>scratch-work/keras-reprex.R</code>.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="appendix.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb62-2"><a href="appendix.html#cb62-2" aria-hidden="true" tabindex="-1"></a>num_features <span class="ot">&lt;-</span> <span class="fu">ncol</span>(dtm_train_matrix)</span>
<span id="cb62-3"><a href="appendix.html#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="appendix.html#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the input layer is implicitly added with input_shape</span></span>
<span id="cb62-5"><a href="appendix.html#cb62-5" aria-hidden="true" tabindex="-1"></a>mlp_model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb62-6"><a href="appendix.html#cb62-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">8</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(num_features)) <span class="sc">%&gt;%</span>   </span>
<span id="cb62-7"><a href="appendix.html#cb62-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb62-8"><a href="appendix.html#cb62-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-9"><a href="appendix.html#cb62-9" aria-hidden="true" tabindex="-1"></a>mlp_model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb62-10"><a href="appendix.html#cb62-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb62-11"><a href="appendix.html#cb62-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb62-12"><a href="appendix.html#cb62-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb62-13"><a href="appendix.html#cb62-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb62-14"><a href="appendix.html#cb62-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-15"><a href="appendix.html#cb62-15" aria-hidden="true" tabindex="-1"></a><span class="co"># use validation data to determine best number of epochs to run our MLP for</span></span>
<span id="cb62-16"><a href="appendix.html#cb62-16" aria-hidden="true" tabindex="-1"></a>mlp_history <span class="ot">&lt;-</span> mlp_model <span class="sc">%&gt;%</span> keras<span class="sc">::</span><span class="fu">fit</span>(</span>
<span id="cb62-17"><a href="appendix.html#cb62-17" aria-hidden="true" tabindex="-1"></a>  dtm_train_matrix, train_targets_numerical,</span>
<span id="cb62-18"><a href="appendix.html#cb62-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">15</span>,</span>
<span id="cb62-19"><a href="appendix.html#cb62-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">1</span>,</span>
<span id="cb62-20"><a href="appendix.html#cb62-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb62-21"><a href="appendix.html#cb62-21" aria-hidden="true" tabindex="-1"></a>) </span></code></pre></div>
<div class="figure"><span id="fig:mlp-history"></span>
<img src="_main_files/figure-html/mlp-history-1.png" alt="Validation data performance over multiple epochs in a MLP model" width="576"  />
<p class="caption">
Figure 7.1: Validation data performance over multiple epochs in a MLP model
</p>
</div>
<p>Figure <a href="appendix.html#fig:mlp-history">7.1</a> shows us how our validation data performs as our MLP performs multiple feedforward/backpropagation epochs. Since we want to minimize our validation data’s loss and maximize our validation data’s accuracy, we observe that out <span class="math inline">\(3\)</span> epochs seems to be reasonable number of times to run our MLP (note that this may change due to the issue mentioned above about random reproducibility in <code>keras</code>).</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="appendix.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb63-2"><a href="appendix.html#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="appendix.html#cb63-3" aria-hidden="true" tabindex="-1"></a>mlp_model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb63-4"><a href="appendix.html#cb63-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">8</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(num_features)) <span class="sc">%&gt;%</span>   </span>
<span id="cb63-5"><a href="appendix.html#cb63-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb63-6"><a href="appendix.html#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="appendix.html#cb63-7" aria-hidden="true" tabindex="-1"></a>mlp_model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb63-8"><a href="appendix.html#cb63-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb63-9"><a href="appendix.html#cb63-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb63-10"><a href="appendix.html#cb63-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb63-11"><a href="appendix.html#cb63-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-12"><a href="appendix.html#cb63-12" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb63-13"><a href="appendix.html#cb63-13" aria-hidden="true" tabindex="-1"></a>mlp_model <span class="sc">%&gt;%</span> keras<span class="sc">::</span><span class="fu">fit</span>(<span class="at">x =</span> dtm_train_matrix, <span class="at">y =</span> train_targets_numerical,</span>
<span id="cb63-14"><a href="appendix.html#cb63-14" aria-hidden="true" tabindex="-1"></a>                         <span class="at">epochs =</span> <span class="dv">3</span>, <span class="at">batch_size =</span> <span class="dv">1</span>)</span>
<span id="cb63-15"><a href="appendix.html#cb63-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-16"><a href="appendix.html#cb63-16" aria-hidden="true" tabindex="-1"></a>mlp_results <span class="ot">&lt;-</span> mlp_model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(<span class="at">x =</span> dtm_test_matrix, test_targets_numerical)</span>
<span id="cb63-17"><a href="appendix.html#cb63-17" aria-hidden="true" tabindex="-1"></a>mlp_results</span></code></pre></div>
<pre><code>##      loss  accuracy 
## 0.6003342 0.6684073</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="appendix.html#cb65-1" aria-hidden="true" tabindex="-1"></a>mlp_predicted_values <span class="ot">&lt;-</span> mlp_model <span class="sc">%&gt;%</span> <span class="fu">predict_classes</span>(dtm_test_matrix, <span class="at">batch_size =</span> <span class="dv">1</span>)</span>
<span id="cb65-2"><a href="appendix.html#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="appendix.html#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix</span></span>
<span id="cb65-4"><a href="appendix.html#cb65-4" aria-hidden="true" tabindex="-1"></a>mlp_confusion_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(mlp_predicted_values),</span>
<span id="cb65-5"><a href="appendix.html#cb65-5" aria-hidden="true" tabindex="-1"></a>                                              test_targets_categorical,</span>
<span id="cb65-6"><a href="appendix.html#cb65-6" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>))</span>
<span id="cb65-7"><a href="appendix.html#cb65-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-8"><a href="appendix.html#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix table</span></span>
<span id="cb65-9"><a href="appendix.html#cb65-9" aria-hidden="true" tabindex="-1"></a>mlp_confusion_matrix<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 108  50
##         1  77 148</code></pre>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="appendix.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># around 70% accuracy</span></span>
<span id="cb67-2"><a href="appendix.html#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(mlp_confusion_matrix<span class="sc">$</span>overall[<span class="dv">1</span>], <span class="dv">3</span>) </span></code></pre></div>
<pre><code>## Accuracy 
##    0.668</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="appendix.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% CI</span></span>
<span id="cb69-2"><a href="appendix.html#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(mlp_confusion_matrix<span class="sc">$</span>overall[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">3</span>)  </span></code></pre></div>
<pre><code>## AccuracyLower AccuracyUpper 
##         0.619         0.715</code></pre>
<p>Our MLP performs reasonably well with an accuracy of around <span class="math inline">\(70\%\)</span> (and a 95% confidence interval around <span class="math inline">\([65\%, 75\%]\)</span>).</p>
</div>
<div id="recurrent-neural-network" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Recurrent Neural Network</h3>
<p>The <code>keras</code> package is used to fit a Recurrent Neural Network (RNN). Here, a RNN with one input layer, one long short-term memory (LSTM)<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> layer, and one output layer is created. The <code>keras</code> requires features to be stored in matrices and targets to be numerical. Note that a RNN requires a different type of DTM representation (as mentioned previously) where each word must be given a unique numerical ID so that a sense of sequence can be maintained.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="appendix.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the number of words to include in our vocab (keep the same as our other vocab)</span></span>
<span id="cb71-2"><a href="appendix.html#cb71-2" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="ot">&lt;-</span> <span class="dv">806</span></span>
<span id="cb71-3"><a href="appendix.html#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="appendix.html#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co"># note that this tokenizer is based on the training data</span></span>
<span id="cb71-5"><a href="appendix.html#cb71-5" aria-hidden="true" tabindex="-1"></a>rnn_tokenizer <span class="ot">&lt;-</span> <span class="fu">text_tokenizer</span>(<span class="at">num_words =</span> vocab_size) <span class="sc">%&gt;%</span> </span>
<span id="cb71-6"><a href="appendix.html#cb71-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit_text_tokenizer</span>(politifact_df_train<span class="sc">$</span>article_text)</span>
<span id="cb71-7"><a href="appendix.html#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="appendix.html#cb71-8" aria-hidden="true" tabindex="-1"></a>train_sequences <span class="ot">&lt;-</span> <span class="fu">texts_to_sequences</span>(rnn_tokenizer, politifact_df_train<span class="sc">$</span>article_text)</span>
<span id="cb71-9"><a href="appendix.html#cb71-9" aria-hidden="true" tabindex="-1"></a>test_sequences <span class="ot">&lt;-</span> <span class="fu">texts_to_sequences</span>(rnn_tokenizer, politifact_df_test<span class="sc">$</span>article_text)</span>
<span id="cb71-10"><a href="appendix.html#cb71-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-11"><a href="appendix.html#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="co"># pad sequences (add 0&#39;s to the end) for documents that are short</span></span>
<span id="cb71-12"><a href="appendix.html#cb71-12" aria-hidden="true" tabindex="-1"></a>rnn_x_train <span class="ot">&lt;-</span>  <span class="fu">pad_sequences</span>(train_sequences, <span class="at">padding =</span> <span class="st">&quot;post&quot;</span>)</span>
<span id="cb71-13"><a href="appendix.html#cb71-13" aria-hidden="true" tabindex="-1"></a>rnn_x_test <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(test_sequences, <span class="at">padding =</span> <span class="st">&quot;post&quot;</span>) </span>
<span id="cb71-14"><a href="appendix.html#cb71-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-15"><a href="appendix.html#cb71-15" aria-hidden="true" tabindex="-1"></a><span class="co"># take a look at what a RNN DTM looks like</span></span>
<span id="cb71-16"><a href="appendix.html#cb71-16" aria-hidden="true" tabindex="-1"></a>rnn_x_train[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]   57   48  146  165    3
## [2,]   53  135    0    0    0
## [3,]  219  136    9  368  414
## [4,]    1   11   15  415    0
## [5,]    1  485  137  573   98</code></pre>
<p>Now that we have our features in the right form (they have a notion of sequence), we can fit our RNN.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="appendix.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb73-2"><a href="appendix.html#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="appendix.html#cb73-3" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>()  <span class="sc">%&gt;%</span></span>
<span id="cb73-4"><a href="appendix.html#cb73-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> vocab_size, <span class="at">output_dim =</span> <span class="dv">64</span>) <span class="sc">%&gt;%</span>  </span>
<span id="cb73-5"><a href="appendix.html#cb73-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">128</span>, <span class="at">dropout =</span> <span class="fl">0.2</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb73-6"><a href="appendix.html#cb73-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb73-7"><a href="appendix.html#cb73-7" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb73-8"><a href="appendix.html#cb73-8" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb73-9"><a href="appendix.html#cb73-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb73-10"><a href="appendix.html#cb73-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb73-11"><a href="appendix.html#cb73-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb73-12"><a href="appendix.html#cb73-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-13"><a href="appendix.html#cb73-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-14"><a href="appendix.html#cb73-14" aria-hidden="true" tabindex="-1"></a>batch_size       <span class="ot">&lt;-</span> <span class="dv">64</span></span>
<span id="cb73-15"><a href="appendix.html#cb73-15" aria-hidden="true" tabindex="-1"></a>epochs           <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb73-16"><a href="appendix.html#cb73-16" aria-hidden="true" tabindex="-1"></a>validation_split <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb73-17"><a href="appendix.html#cb73-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-18"><a href="appendix.html#cb73-18" aria-hidden="true" tabindex="-1"></a>rnn_history <span class="ot">&lt;-</span> rnn_model <span class="sc">%&gt;%</span> keras<span class="sc">::</span><span class="fu">fit</span>(</span>
<span id="cb73-19"><a href="appendix.html#cb73-19" aria-hidden="true" tabindex="-1"></a>  rnn_x_train, train_targets_numerical,</span>
<span id="cb73-20"><a href="appendix.html#cb73-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size,</span>
<span id="cb73-21"><a href="appendix.html#cb73-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> epochs,</span>
<span id="cb73-22"><a href="appendix.html#cb73-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> validation_split</span>
<span id="cb73-23"><a href="appendix.html#cb73-23" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:rnn-history"></span>
<img src="_main_files/figure-html/rnn-history-1.png" alt="Validation data performance over multiple epochs in a RNN model" width="576"  />
<p class="caption">
Figure 7.2: Validation data performance over multiple epochs in a RNN model
</p>
</div>
<p>Figure <a href="appendix.html#fig:rnn-history">7.2</a> shows us how our validation data performs as our RNN performs multiple feedforward/backpropagation epochs Since we want to minimize our validation data’s loss and maximize our validation data’s accuracy, we observe that out <span class="math inline">\(3\)</span> epochs seems to be reasonable number of times to run our MLP (note that this may change due to the issue mentioned above about random reproducibility in <code>keras</code>).</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="appendix.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># recompile model</span></span>
<span id="cb74-2"><a href="appendix.html#cb74-2" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>()  <span class="sc">%&gt;%</span></span>
<span id="cb74-3"><a href="appendix.html#cb74-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> vocab_size, <span class="at">output_dim =</span> <span class="dv">64</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb74-4"><a href="appendix.html#cb74-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">128</span>, <span class="at">dropout =</span> <span class="fl">0.2</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb74-5"><a href="appendix.html#cb74-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb74-6"><a href="appendix.html#cb74-6" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb74-7"><a href="appendix.html#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="appendix.html#cb74-8" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb74-9"><a href="appendix.html#cb74-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb74-10"><a href="appendix.html#cb74-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb74-11"><a href="appendix.html#cb74-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb74-12"><a href="appendix.html#cb74-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-13"><a href="appendix.html#cb74-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-14"><a href="appendix.html#cb74-14" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="sc">%&gt;%</span> keras<span class="sc">::</span><span class="fu">fit</span>(rnn_x_train, train_targets_numerical, <span class="at">epochs =</span> <span class="dv">3</span>, <span class="at">batch_size =</span> batch_size)</span>
<span id="cb74-15"><a href="appendix.html#cb74-15" aria-hidden="true" tabindex="-1"></a>rnn_results <span class="ot">&lt;-</span> rnn_model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(rnn_x_test, test_targets_numerical)</span>
<span id="cb74-16"><a href="appendix.html#cb74-16" aria-hidden="true" tabindex="-1"></a>rnn_predicted_values <span class="ot">&lt;-</span> rnn_model <span class="sc">%&gt;%</span> <span class="fu">predict_classes</span>(rnn_x_test, <span class="at">batch_size =</span> batch_size)</span>
<span id="cb74-17"><a href="appendix.html#cb74-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-18"><a href="appendix.html#cb74-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb74-19"><a href="appendix.html#cb74-19" aria-hidden="true" tabindex="-1"></a>rnn_confusion_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(rnn_predicted_values),</span>
<span id="cb74-20"><a href="appendix.html#cb74-20" aria-hidden="true" tabindex="-1"></a>                                              test_targets_categorical,</span>
<span id="cb74-21"><a href="appendix.html#cb74-21" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted&quot;</span>, <span class="st">&quot;Actual&quot;</span>))</span>
<span id="cb74-22"><a href="appendix.html#cb74-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-23"><a href="appendix.html#cb74-23" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix table</span></span>
<span id="cb74-24"><a href="appendix.html#cb74-24" aria-hidden="true" tabindex="-1"></a>rnn_confusion_matrix<span class="sc">$</span>table</span></code></pre></div>
<pre><code>##          Actual
## Predicted   0   1
##         0 135  70
##         1  50 128</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="appendix.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># around 68% accuracy</span></span>
<span id="cb76-2"><a href="appendix.html#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rnn_confusion_matrix<span class="sc">$</span>overall[<span class="dv">1</span>], <span class="dv">3</span>) </span></code></pre></div>
<pre><code>## Accuracy 
##    0.687</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="appendix.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% CI  around (0.64, 0.73)</span></span>
<span id="cb78-2"><a href="appendix.html#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rnn_confusion_matrix<span class="sc">$</span>overall[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">3</span>)  </span></code></pre></div>
<pre><code>## AccuracyLower AccuracyUpper 
##         0.638         0.733</code></pre>
<p>Our RNN performs reasonably well with an accuracy of around <span class="math inline">\(68\%\)</span> (and a 95% confidence interval around <span class="math inline">\([63\%, 73\%]\)</span>).</p>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="27">
<li id="fn27"><p><span class="citation"><a href="references.html#ref-AnalyzingTextsText2vec" role="doc-biblioref"><span>“Analyzing <span>Texts</span> with the Text2vec Package”</span></a> (<a href="references.html#ref-AnalyzingTextsText2vec" role="doc-biblioref">n.d.</a>)</span><a href="appendix.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p><span class="citation"><a href="references.html#ref-nagpalL1L2Regularization2017" role="doc-biblioref">Nagpal</a> (<a href="references.html#ref-nagpalL1L2Regularization2017" role="doc-biblioref">2017</a>)</span><a href="appendix.html#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn29"><p><span class="citation"><a href="references.html#ref-CrossvalidationStatistics2020" role="doc-biblioref"><span>“Cross-Validation (Statistics)”</span></a> (<a href="references.html#ref-CrossvalidationStatistics2020" role="doc-biblioref">2020</a>)</span><a href="appendix.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>“LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs.” <span class="citation"><a href="references.html#ref-LongShorttermMemory2020" role="doc-biblioref"><span>“Long Short-Term Memory”</span></a> (<a href="references.html#ref-LongShorttermMemory2020" role="doc-biblioref">2020</a>)</span><a href="appendix.html#fnref30" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conclusion.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
