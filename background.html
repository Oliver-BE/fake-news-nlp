<!DOCTYPE html>
<html lang="" xml:lang="">

<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Background | Classifying Fake News using NLP and ML</title>
  <meta name="description" content="An introductory look at using NLP and ML to classify news articles" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Background | Classifying Fake News using NLP and ML" />
  <meta property="og:type" content="book" />


  <meta property="og:description" content="An introductory look at using NLP and ML to classify news articles" />


  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Background | Classifying Fake News using NLP and ML" />

  <meta name="twitter:description" content="An introductory look at using NLP and ML to classify news articles" />


  <meta name="author" content="Oliver Baldwin Edwards" />


  <meta name="date" content="2020-12-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />


  <link rel="prev" href="introduction.html" />
  <link rel="next" href="data.html" />
  <script src="libs/header-attrs-2.5/header-attrs.js"></script>
  <script src="libs/jquery-2.2.3/jquery.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />


  <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
  <link rel="manifest" href="favicon/site.webmanifest">






  <link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


  <style type="text/css">
    pre>code.sourceCode {
      white-space: pre;
      position: relative;
    }

    pre>code.sourceCode>span {
      display: inline-block;
      line-height: 1.25;
    }

    pre>code.sourceCode>span:empty {
      height: 1.2em;
    }

    .sourceCode {
      overflow: visible;
    }

    code.sourceCode>span {
      color: inherit;
      text-decoration: inherit;
    }

    pre.sourceCode {
      margin: 0;
    }

    @media screen {
      div.sourceCode {
        overflow: auto;
      }
    }

    @media print {
      pre>code.sourceCode {
        white-space: pre-wrap;
      }

      pre>code.sourceCode>span {
        text-indent: -5em;
        padding-left: 5em;
      }
    }

    pre.numberSource code {
      counter-reset: source-line 0;
    }

    pre.numberSource code>span {
      position: relative;
      left: -4em;
      counter-increment: source-line;
    }

    pre.numberSource code>span>a:first-child::before {
      content: counter(source-line);
      position: relative;
      left: -1em;
      text-align: right;
      vertical-align: baseline;
      border: none;
      display: inline-block;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -khtml-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      padding: 0 4px;
      width: 4em;
      color: #aaaaaa;
    }

    pre.numberSource {
      margin-left: 3em;
      border-left: 1px solid #aaaaaa;
      padding-left: 4px;
    }

    div.sourceCode {}

    @media screen {
      pre>code.sourceCode>span>a:first-child::before {
        text-decoration: underline;
      }
    }

    code span.al {
      color: #ff0000;
      font-weight: bold;
    }

    /* Alert */
    code span.an {
      color: #60a0b0;
      font-weight: bold;
      font-style: italic;
    }

    /* Annotation */
    code span.at {
      color: #7d9029;
    }

    /* Attribute */
    code span.bn {
      color: #40a070;
    }

    /* BaseN */
    code span.bu {}

    /* BuiltIn */
    code span.cf {
      color: #007020;
      font-weight: bold;
    }

    /* ControlFlow */
    code span.ch {
      color: #4070a0;
    }

    /* Char */
    code span.cn {
      color: #880000;
    }

    /* Constant */
    code span.co {
      color: #60a0b0;
      font-style: italic;
    }

    /* Comment */
    code span.cv {
      color: #60a0b0;
      font-weight: bold;
      font-style: italic;
    }

    /* CommentVar */
    code span.do {
      color: #ba2121;
      font-style: italic;
    }

    /* Documentation */
    code span.dt {
      color: #902000;
    }

    /* DataType */
    code span.dv {
      color: #40a070;
    }

    /* DecVal */
    code span.er {
      color: #ff0000;
      font-weight: bold;
    }

    /* Error */
    code span.ex {}

    /* Extension */
    code span.fl {
      color: #40a070;
    }

    /* Float */
    code span.fu {
      color: #06287e;
    }

    /* Function */
    code span.im {}

    /* Import */
    code span.in {
      color: #60a0b0;
      font-weight: bold;
      font-style: italic;
    }

    /* Information */
    code span.kw {
      color: #007020;
      font-weight: bold;
    }

    /* Keyword */
    code span.op {
      color: #666666;
    }

    /* Operator */
    code span.ot {
      color: #007020;
    }

    /* Other */
    code span.pp {
      color: #bc7a00;
    }

    /* Preprocessor */
    code span.sc {
      color: #4070a0;
    }

    /* SpecialChar */
    code span.ss {
      color: #bb6688;
    }

    /* SpecialString */
    code span.st {
      color: #4070a0;
    }

    /* String */
    code span.va {
      color: #19177c;
    }

    /* Variable */
    code span.vs {
      color: #4070a0;
    }

    /* VerbatimString */
    code span.wa {
      color: #60a0b0;
      font-weight: bold;
      font-style: italic;
    }

    /* Warning */
  </style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
        <ul class="summary">
          <li><a href="./">Classifying Fake News using NLP and ML</a></li>

          <li class="divider"></li>
          <!-- <ul class="summary"> -->
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Preface</a></li>
          <li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i
                class="fa fa-check"></i><b>1</b> Introduction</a></li>
          <li class="chapter" data-level="2" data-path="background.html"><a href="background.html"><i
                class="fa fa-check"></i><b>2</b> Background</a>
            <ul>
              <li class="chapter" data-level="2.1" data-path="background.html"><a
                  href="background.html#natural-language-processing"><i class="fa fa-check"></i><b>2.1</b> Natural
                  Language Processing</a>
                <ul>
                  <li class="chapter" data-level="2.1.1" data-path="background.html"><a
                      href="background.html#reducing-a-vocabulary-with-lemmatization-and-stop-words"><i
                        class="fa fa-check"></i><b>2.1.1</b> Reducing a Vocabulary with Lemmatization and Stop
                      Words</a>
                  </li>
                  <li class="chapter" data-level="2.1.2" data-path="background.html"><a
                      href="background.html#bag-of-words"><i class="fa fa-check"></i><b>2.1.2</b> Bag-of-Words Model
                      for
                      Feature Extraction</a></li>
                </ul>
              </li>
              <li class="chapter" data-level="2.2" data-path="background.html"><a
                  href="background.html#deep-learning-models"><i class="fa fa-check"></i><b>2.2</b> Deep Learning
                  Models</a>
                <ul>
                  <li class="chapter" data-level="2.2.1" data-path="background.html"><a
                      href="background.html#multilayer-perceptrons"><i class="fa fa-check"></i><b>2.2.1</b> Multilayer
                      Perceptrons</a></li>
                  <li class="chapter" data-level="2.2.2" data-path="background.html"><a
                      href="background.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2.2</b>
                      Recurrent
                      Neural Networks</a></li>
                </ul>
              </li>
            </ul>
          </li>
          <li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i
                class="fa fa-check"></i><b>3</b> Data</a>
            <ul>
              <li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#data-cleaning"><i
                    class="fa fa-check"></i><b>3.1</b> Data Cleaning</a></li>
              <li class="chapter" data-level="3.2" data-path="data.html"><a
                  href="data.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>3.2</b> Exploratory Data
                  Analysis</a></li>
            </ul>
          </li>
          <li class="chapter" data-level="4" data-path="model-fitting.html"><a href="model-fitting.html"><i
                class="fa fa-check"></i><b>4</b> Model Fitting</a></li>
          <li class="chapter" data-level="5" data-path="results.html"><a href="results.html"><i
                class="fa fa-check"></i><b>5</b> Results</a></li>
          <li class="chapter" data-level="6" data-path="conclusion.html"><a href="conclusion.html"><i
                class="fa fa-check"></i><b>6</b> Conclusion</a>
            <ul>
              <li class="chapter" data-level="6.1" data-path="conclusion.html"><a href="conclusion.html#limitations"><i
                    class="fa fa-check"></i><b>6.1</b> Limitations</a></li>
              <li class="chapter" data-level="6.2" data-path="conclusion.html"><a href="conclusion.html#future-work"><i
                    class="fa fa-check"></i><b>6.2</b> Future Work</a></li>
            </ul>
          </li>
          <li class="chapter" data-level="7" data-path="appendix.html"><a href="appendix.html"><i
                class="fa fa-check"></i><b>7</b> Appendix</a>
            <ul>
              <li class="chapter" data-level="7.1" data-path="appendix.html"><a href="appendix.html#appendix-data"><i
                    class="fa fa-check"></i><b>7.1</b> Data Wrangling/Feature Extraction</a>
                <ul>
                  <li class="chapter" data-level="7.1.1" data-path="appendix.html"><a
                      href="appendix.html#basic-data-wrangling"><i class="fa fa-check"></i><b>7.1.1</b> Basic Data
                      Wrangling</a></li>
                  <li class="chapter" data-level="7.1.2" data-path="appendix.html"><a
                      href="appendix.html#cleaning-the-textreducing-the-vocabulary-size"><i
                        class="fa fa-check"></i><b>7.1.2</b> Cleaning the Text/Reducing the Vocabulary Size</a></li>
                  <li class="chapter" data-level="7.1.3" data-path="appendix.html"><a
                      href="appendix.html#traintest-split"><i class="fa fa-check"></i><b>7.1.3</b> Train/Test
                      Split</a>
                  </li>
                  <li class="chapter" data-level="7.1.4" data-path="appendix.html"><a
                      href="appendix.html#feature-extraction"><i class="fa fa-check"></i><b>7.1.4</b> Creating a
                      Document Term Matrix</a></li>
                </ul>
              </li>
              <li class="chapter" data-level="7.2" data-path="appendix.html"><a
                  href="appendix.html#appendix-initial-models"><i class="fa fa-check"></i><b>7.2</b> Initial Model
                  Fitting</a>
                <ul>
                  <li class="chapter" data-level="7.2.1" data-path="appendix.html"><a
                      href="appendix.html#naive-bayes"><i class="fa fa-check"></i><b>7.2.1</b> Naive Bayes</a></li>
                  <li class="chapter" data-level="7.2.2" data-path="appendix.html"><a
                      href="appendix.html#basic-logistic-regression"><i class="fa fa-check"></i><b>7.2.2</b> Basic
                      Logistic Regression</a></li>
                  <li class="chapter" data-level="7.2.3" data-path="appendix.html"><a
                      href="appendix.html#logistic-regresion-with-l1-penalty-lasso-regression"><i
                        class="fa fa-check"></i><b>7.2.3</b> Logistic Regresion with L1 penalty (Lasso Regression)</a>
                  </li>
                  <li class="chapter" data-level="7.2.4" data-path="appendix.html"><a
                      href="appendix.html#support-vector-machine"><i class="fa fa-check"></i><b>7.2.4</b> Support
                      Vector
                      Machine</a></li>
                  <li class="chapter" data-level="7.2.5" data-path="appendix.html"><a
                      href="appendix.html#random-forest"><i class="fa fa-check"></i><b>7.2.5</b> Random Forest</a>
                  </li>
                </ul>
              </li>
              <li class="chapter" data-level="7.3" data-path="appendix.html"><a
                  href="appendix.html#appendix-deeplearning-models"><i class="fa fa-check"></i><b>7.3</b> Deep
                  Learning
                  Model Fitting</a>
                <ul>
                  <li class="chapter" data-level="7.3.1" data-path="appendix.html"><a
                      href="appendix.html#multilayer-perceptron-neural-network"><i class="fa fa-check"></i><b>7.3.1</b>
                      Multilayer Perceptron Neural Network</a></li>
                  <li class="chapter" data-level="7.3.2" data-path="appendix.html"><a
                      href="appendix.html#recurrent-neural-network"><i class="fa fa-check"></i><b>7.3.2</b> Recurrent
                      Neural Network</a></li>
                </ul>
              </li>
            </ul>
          </li>
          <li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i
                class="fa fa-check"></i>References</a></li>
        </ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Classifying Fake News using NLP and ML</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="background" class="section level1" number="2">
                <h1><span class="header-section-number">2</span> Background</h1>
                <div id="natural-language-processing" class="section level2" number="2.1">
                  <h2><span class="header-section-number">2.1</span> Natural Language Processing</h2>
                  <p>Natural Language Processing (NLP) is the process of helping computers understand and interpret
                    human language (which is hard to do without an inherent knowledge of tone, connotation, sarcasm,
                    etc.).<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> NLP is important because it
                    allows computers to read text (or listen to speech) and interpret what messages are being conveyed.
                    It is used in tasks such as sentiment analysis, language translation, or—in the case of fake
                    news—classification.</p>
                  <p>NLP is advantageous because it is able to analyze language-based data “in a consistent and unbiased
                    way.”<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> Note that the notion of
                    algorithmic bias<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> is a complex topic
                    that is beyond the scope of this project. Thus, for the purposes of this project, the NLP used
                    assumes unbiased classification. Assuming this sort of unbiased classification, using NLP to
                    classify fake news is clearly beneficial.</p>
                  <p>In this project, NLP is used to extract features from a set of PolitiFact claims. Those features,
                    and the associated truth rating of each claim, are then used to fit multiple machine learning
                    algorithms with the end goal of classifying fake news with a high accuracy. In order to extract
                    features from text, lemmatization and stop words are first used to reduce the size of the overall
                    vocabulary. Then, the bag-of-words model, which is represented by a document-term matrix (DTM), is
                    used to create a matrix of features.</p>
                  <div id="reducing-a-vocabulary-with-lemmatization-and-stop-words" class="section level3"
                    number="2.1.1">
                    <h3><span class="header-section-number">2.1.1</span> Reducing a Vocabulary with Lemmatization and
                      Stop Words</h3>
                    <p>In NLP, a text corpus refers to the collection of text documents being used. In the context of
                      this project, the text corpus is all of the PolitiFact claims used. A vocabulary, in NLP, is then
                      the set of unique words used in a text corpus.<a href="#fn10" class="footnote-ref"
                        id="fnref10"><sup>10</sup></a> When extracting features from text in NLP, a common problem is
                      that the vocabulary is too large. This is due to the fact that many words that have very similar
                      meanings (such as “cook,” “cooks,” and “cooking”) are each included as a separate word in the
                      vocabulary. In addition, many words that (in most contexts) don’t carry meaning—such as “and,”
                      “of,” and “the”—significantly increase the vocabulary length. To combat this,
                      stemming/lemmatization and the removal of stop words can be used.</p>
                    <p>Stemming is a process used in NLP that removes the ends of words in order to get rid of words
                      that have derivational affixes.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>
                      There are standard stemming libraries that remove endings of words such as “-ing,” “-es,” and
                      “-ed.” In addition, users can create their own custom functions to remove endings of words that
                      are not included in a stemming library. In stemming, the words “dog,” “dogs,” and “dog’s” would
                      all be cut down to the word “dog.” This helps to reduce the number of words that are counted in a
                      document. However, a word such as “doggie” would likely be missed by stemming (unless a custom
                      function had “-gie” set to be removed). Because of such exceptions, lemmatization is used in this
                      project instead of stemming.</p>
                    <p>Lemmatization has the same end goal of stemming, but uses a vocabulary of base words (lemmas)
                      that words in a document are matched to (and then changed to).<a href="#fn12" class="footnote-ref"
                        id="fnref12"><sup>12</sup></a> An example of when lemmatization is preferable over stemming is
                      with a word such as “better,” which has “good” as its lemma. This sort of word would be missed
                      with stemming, but caught with lemmatization (which would have a dictionary matching these two
                      words together). Another example of when lemmatization is preferable over stemming is with the
                      would “caring.” Lemmatizing “caring” gives us “care,” but stemming would likely give you “car”
                      (since a stemming function would erroneously try to remove just the “-ing”). Lemmatization is
                      useful since it is able to reduce the vocabulary size by condensing words that have different
                      textual representations (but are the same words otherwise) into just one word in a vocabulary.</p>
                    <p>Stop word removal is the process of removing specified words from a vocabulary with the goal of
                      reducing the vocabulary size. Stop words are words such as “and,” “the,” and “as” that and add
                      noise to NLP feature extraction and don’t provide any interesting insights. Custom stop words
                      (depending on the context of the problem at hand) can also be added to a stop words dictionary.
                    </p>
                    <p>A full working example of how to clean a single text entry to do the above discussed
                      lemmatization and stop word removal (as well as some other standard text cleaning operations) can
                      be seen in the code chunk below. A very similar text cleaning function is used to clean the
                      PolitiFact claims in this project (and can be seen in Section <a
                        href="appendix.html#appendix-data">7.1</a>).</p>
                    <div class="sourceCode" id="cb1">
                      <pre
                        class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="background.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is our text input </span></span>
<span id="cb1-2"><a href="background.html#cb1-2" aria-hidden="true" tabindex="-1"></a>trump_claim_raw <span class="ot">&lt;-</span> <span class="st">&quot;&#39;Trump approval rating    better than Obama  and Reagan at </span></span>
<span id="cb1-3"><a href="background.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="st">            same point in their presidencies.&#39;&quot;</span>  </span>
<span id="cb1-4"><a href="background.html#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="background.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to perform text cleaning</span></span>
<span id="cb1-6"><a href="background.html#cb1-6" aria-hidden="true" tabindex="-1"></a>clean_text <span class="ot">&lt;-</span> <span class="cf">function</span>(input_text) {</span>
<span id="cb1-7"><a href="background.html#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create a list of all of the stop words we want to remove from our corpus</span></span>
<span id="cb1-8"><a href="background.html#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note that `stopwords(&quot;en&quot;)` is a pre-defined list of stop words, but we</span></span>
<span id="cb1-9"><a href="background.html#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># can also add custom stop words (here Obama and Reagan are added)</span></span>
<span id="cb1-10"><a href="background.html#cb1-10" aria-hidden="true" tabindex="-1"></a>  all_stops <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Obama&quot;</span>, <span class="st">&quot;Reagan&quot;</span>, <span class="fu">stopwords</span>(<span class="st">&quot;en&quot;</span>))</span>
<span id="cb1-11"><a href="background.html#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="background.html#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove any punctuation from the text</span></span>
<span id="cb1-13"><a href="background.html#cb1-13" aria-hidden="true" tabindex="-1"></a>  output_text <span class="ot">&lt;-</span> <span class="fu">removePunctuation</span>(input_text) <span class="sc">%&gt;%</span> </span>
<span id="cb1-14"><a href="background.html#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove our custom list of stop words</span></span>
<span id="cb1-15"><a href="background.html#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">removeWords</span>(all_stops) <span class="sc">%&gt;%</span> </span>
<span id="cb1-16"><a href="background.html#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make all letters lowercase</span></span>
<span id="cb1-17"><a href="background.html#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tolower</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb1-18"><a href="background.html#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># lemmatize the words in the text </span></span>
<span id="cb1-19"><a href="background.html#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lemmatize_strings</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb1-20"><a href="background.html#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove any numbers in the text</span></span>
<span id="cb1-21"><a href="background.html#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">removeNumbers</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb1-22"><a href="background.html#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get rid of any extra white space</span></span>
<span id="cb1-23"><a href="background.html#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stripWhitespace</span>()</span>
<span id="cb1-24"><a href="background.html#cb1-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-25"><a href="background.html#cb1-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output_text)</span>
<span id="cb1-26"><a href="background.html#cb1-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-27"><a href="background.html#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="background.html#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># apply the text cleaning function</span></span>
<span id="cb1-29"><a href="background.html#cb1-29" aria-hidden="true" tabindex="-1"></a>trump_claim_clean <span class="ot">&lt;-</span> <span class="fu">clean_text</span>(trump_claim_raw)</span>
<span id="cb1-30"><a href="background.html#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># note how &quot;better&quot; becomes good and &quot;presidencies&quot; becomes &quot;presidency&quot;</span></span>
<span id="cb1-31"><a href="background.html#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># also notice that &quot;Obama&quot; and &quot;Reagan&quot; were removed</span></span>
<span id="cb1-32"><a href="background.html#cb1-32" aria-hidden="true" tabindex="-1"></a>trump_claim_clean</span></code></pre>
                    </div>
                    <pre><code>## [1] &quot;trump approval rate good point presidency&quot;</code></pre>
                  </div>
                  <div id="bag-of-words" class="section level3" number="2.1.2">
                    <h3><span class="header-section-number">2.1.2</span> Bag-of-Words Model for Feature Extraction</h3>
                    <p></p>
                    <p>Once a vocabulary has been created—and its size reduced—features can be extracted using the
                      bag-of-words model. The bag-of-words model is a way to extract features from text by describing
                      the occurrence of each word in the vocabulary within a single document (and discarding any
                      information about the order of those words).<a href="#fn13" class="footnote-ref"
                        id="fnref13"><sup>13</sup></a> Each document in a text corpus (each PolitiFact claim) consists
                      of a score for each word in the vocabulary. In the most simple case, that score is simply the
                      number of times each word in a document occurs in the vocabulary. Note that there are many
                      alternative scoring metrics here, one such being the term frequency-inverse document frequency
                      (tf-idf).<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> (Tf-idf is used as
                      the scoring metric in this project.)</p>
                    <p>In practice, bag-of-words feature extraction is represented by a document-term matrix (DTM). A
                      DTM is “a matrix that describes the frequency of terms that occur in a collection of documents”
                      where “rows correspond to documents in the collection and columns correspond to terms.”<a
                        href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> To best illustrate this, an
                      example of creating a DTM (using term frequency to fill in cell values) with the
                      <code>text2vec</code> package can be seen in the code chunk below. Here, we can see a basic
                      example of how to extract features from an initially uncleaned text corpus by first reducing the
                      vocabulary size and then creating a DTM. The resulting DTM can be seen in Table <a
                        href="background.html#tab:sample-dtm-table">2.1</a>.
                    </p>
                    <div class="sourceCode" id="cb3">
                      <pre
                        class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="background.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># our initial text corpus</span></span>
<span id="cb3-2"><a href="background.html#cb3-2" aria-hidden="true" tabindex="-1"></a>car_text <span class="ot">&lt;-</span> <span class="fu">rbind.data.frame</span>(<span class="st">&quot;Bob really likes cars&quot;</span>, </span>
<span id="cb3-3"><a href="background.html#cb3-3" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Sally really, really does not like cars&quot;</span>,</span>
<span id="cb3-4"><a href="background.html#cb3-4" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Sally only likes 3 types of cars&quot;</span>)</span>
<span id="cb3-5"><a href="background.html#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="background.html#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># add unique document ids </span></span>
<span id="cb3-7"><a href="background.html#cb3-7" aria-hidden="true" tabindex="-1"></a>car_text <span class="ot">&lt;-</span> <span class="fu">cbind</span>(car_text, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb3-8"><a href="background.html#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(car_text) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;text&quot;</span>, <span class="st">&quot;id&quot;</span>)</span>
<span id="cb3-9"><a href="background.html#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="background.html#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># clean the text using the same cleaning function from before</span></span>
<span id="cb3-11"><a href="background.html#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># in order to reduce the vocabulary size</span></span>
<span id="cb3-12"><a href="background.html#cb3-12" aria-hidden="true" tabindex="-1"></a>car_text <span class="ot">&lt;-</span> car_text <span class="sc">%&gt;%</span> </span>
<span id="cb3-13"><a href="background.html#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">cleaned_text =</span> <span class="fu">clean_text</span>(text))</span>
<span id="cb3-14"><a href="background.html#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="background.html#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># tokenize the text into individual words </span></span>
<span id="cb3-16"><a href="background.html#cb3-16" aria-hidden="true" tabindex="-1"></a>tokenizer_car <span class="ot">&lt;-</span> <span class="fu">word_tokenizer</span>(car_text<span class="sc">$</span>cleaned_text) <span class="sc">%&gt;%</span> </span>
<span id="cb3-17"><a href="background.html#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">itoken</span>(<span class="at">ids =</span> car_text<span class="sc">$</span>id, <span class="at">progressbar =</span> <span class="cn">FALSE</span>)</span>
<span id="cb3-18"><a href="background.html#cb3-18" aria-hidden="true" tabindex="-1"></a>         </span>
<span id="cb3-19"><a href="background.html#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># create our vocabulary using the tokenized words</span></span>
<span id="cb3-20"><a href="background.html#cb3-20" aria-hidden="true" tabindex="-1"></a>car_vocabulary <span class="ot">&lt;-</span> <span class="fu">create_vocabulary</span>(tokenizer_car) </span>
<span id="cb3-21"><a href="background.html#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="background.html#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># create a tokenizer using each word in the vocabulary</span></span>
<span id="cb3-23"><a href="background.html#cb3-23" aria-hidden="true" tabindex="-1"></a>vocabulary_vectorizer <span class="ot">&lt;-</span> <span class="fu">vocab_vectorizer</span>(car_vocabulary) </span>
<span id="cb3-24"><a href="background.html#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="background.html#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co"># finally, create our DTM</span></span>
<span id="cb3-26"><a href="background.html#cb3-26" aria-hidden="true" tabindex="-1"></a>car_dtm <span class="ot">&lt;-</span> <span class="fu">create_dtm</span>(tokenizer_car, vocabulary_vectorizer)</span>
<span id="cb3-27"><a href="background.html#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the DTM to a matrix so that we can print it</span></span>
<span id="cb3-28"><a href="background.html#cb3-28" aria-hidden="true" tabindex="-1"></a>car_dtm <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(car_dtm)</span></code></pre>
                    </div>
                    <table>
                      <caption>
                        <span id="tab:sample-dtm-table">Table 2.1: </span>An example DTM using a term frequency
                        scoring metric
                      </caption>
                      <thead>
                        <tr>
                          <th style="text-align:left;">
                          </th>
                          <th style="text-align:right;">
                            bob
                          </th>
                          <th style="text-align:right;">
                            type
                          </th>
                          <th style="text-align:right;">
                            sally
                          </th>
                          <th style="text-align:right;">
                            car
                          </th>
                          <th style="text-align:right;">
                            like
                          </th>
                          <th style="text-align:right;">
                            really
                          </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="text-align:left;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            0
                          </td>
                          <td style="text-align:right;">
                            0
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                        </tr>
                        <tr>
                          <td style="text-align:left;">
                            2
                          </td>
                          <td style="text-align:right;">
                            0
                          </td>
                          <td style="text-align:right;">
                            0
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            2
                          </td>
                        </tr>
                        <tr>
                          <td style="text-align:left;">
                            3
                          </td>
                          <td style="text-align:right;">
                            0
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            1
                          </td>
                          <td style="text-align:right;">
                            0
                          </td>
                        </tr>
                      </tbody>
                    </table>
                    <p>In Table <a href="background.html#tab:sample-dtm-table">2.1</a> we observe that the vocabulary in
                      this example has been trimmed to six words. Words such as “does,” “of,” and “only” have been
                      removed and words such as “likes” have been lemmatized to “like.” Each row represents each of
                      document and each column represents a word in the vocabulary. Each cell tells us how many times
                      each vocabulary word appears in a specific document. For example, the first document has one
                      occurrence of “really,” the second document has two occurrences, and the third document has none.
                    </p>
                  </div>
                </div>
                <div id="deep-learning-models" class="section level2" number="2.2">
                  <h2><span class="header-section-number">2.2</span> Deep Learning Models</h2>
                  <div id="multilayer-perceptrons" class="section level3" number="2.2.1">
                    <h3><span class="header-section-number">2.2.1</span> Multilayer Perceptrons</h3>
                    <p>Deep learning is a subfield of machine learning that is concerned with artificial neural
                      networks, which are algorithms inspired by the human brain. Neural networks—which is what deep
                      learning models refer to—are beneficial because results tend to get better with more data and
                      larger models (at the expense of computation and run-time). They are also able to “perform
                      automatic feature extraction from raw data,” which makes them well suited to NLP tasks.<a
                        href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
                    <p>To understand how neural networks work we must first understand their building blocks,
                      perceptrons. A perceptron is made up of four parts: input values, weights and bias, a net input
                      function, and an activation function. This can be seen illustrated in Figure <a
                        href="background.html#fig:perceptron-ex">2.1</a>.<a href="#fn17" class="footnote-ref"
                        id="fnref17"><sup>17</sup></a></p>
                    <div class="figure" style="text-align: center"><span id="fig:perceptron-ex"></span>
                      <img src="images/perceptron-example.jpg" alt="A diagram of a basic perceptron" width="90%" />
                      <p class="caption">
                        Figure 2.1: A diagram of a basic perceptron
                      </p>
                    </div>
                    <p>The input values, represented by the first row of nodes in the above figure, are combined with
                      the weight values in the second row of nodes (weight values are initially randomized). The
                      perceptron then receives this net input and passes it to the net input function. This net input is
                      passed to the activation function which decides (based on the net input) whether or not to
                      generate a <span class="math inline">\(+1\)</span> or a <span class="math inline">\(-1\)</span>.
                      This generated output is then the predicted class label of the example. During the learning phase,
                      the predicted class labels are used to calculate the error of each prediction and update the
                      weights accordingly.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></p>
                    <p>When more than one perceptron is connected, stacked in several layers, a neural network is
                      created. In their most basic form, neural networks consist of an input layer of features, a hidden
                      layer, and an output layer. A multilayer perceptron (MLP) is a neural network with at least one
                      hidden layer. Figure <a href="background.html#fig:nn-ex">2.2</a> displays a MLP where each node in
                      the input and hidden layer represent an individual perceptron.<a href="#fn19" class="footnote-ref"
                        id="fnref19"><sup>19</sup></a> Each perceptron in each layer is connected to every other
                      perceptron in the following layer with weighted edges. Because there can be a large number of
                      nodes in each layer, a MLP can quickly become complex. In a similar way to individual perceptrons,
                      output values are predicted by stepping through the neural network during the feedforward phase.
                      Edge weights between neurons are then updated during training through a process called
                      backpropagation.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> This
                      feedforward and backpropogation process repeats for a user-defined number of times. Normally, when
                      evaluating a neural network, you should repeat this process until the edge weights are able to
                      maximize classification accuracy on a validation set.</p>
                    <div class="figure" style="text-align: center"><span id="fig:nn-ex"></span>
                      <img src="images/neural-network-example.png" alt="A diagram of a multilayer perceptron"
                        width="70%" />
                      <p class="caption">
                        Figure 2.2: A diagram of a multilayer perceptron
                      </p>
                    </div>
                    <p>Setting the number of hidden layers in a MLP (as well as the number of nodes in each hidden
                      layer) is an issue of model tuning and can vary from model to model. However, there is rarely much
                      practical need to have more than two hidden layers, and one is sufficient in most problems.<a
                        href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> There is no rule for how many
                      nodes each layer should have, but a general rule of thumb is this: the input layer should have as
                      many nodes as there are features and a hidden layer should have the average of the number of nodes
                      in the input layer and the number of nodes in the output layer.</p>
                  </div>
                  <div id="recurrent-neural-networks" class="section level3" number="2.2.2">
                    <h3><span class="header-section-number">2.2.2</span> Recurrent Neural Networks</h3>
                    <p>Recurrent neural networks (RNN) are designed for problems with a notion of sequence and add a
                      representation of memory to a neural network. For this reason, RNNs are well suited to NLP tasks
                      where the sequencing of words in a sentence matters. RNNs are able to keep track of sequencing by
                      allowing neurons to pass values sideways within a given layer (in addition to being able pass
                      values forwards as normal).<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></p>
                    <p>It’s important to note that since RNNs require a notion of sequence, the bag-of-words model
                      mentioned previously needs to be slightly tweaked. In RNNs, each word in the vocabulary needs its
                      own individual identifier (in the form of an integer). Then, after text cleaning, documents are
                      simply translated from word to (id) number. This can still be represented by a DTM where the
                      number of columns represents the maximum number of words found in a document within the text
                      corpus and the number of rows represents the total number of documents. Each cell value is filled
                      with the numerical representation of each word in a document (in the original order they appeared
                      in). If a given document is shorter than the maximum document length, then 0’s are simply added to
                      the end of it.</p>
                    <div style="page-break-after: always;"></div>
                  </div>
                </div>
              </div>
              <div class="footnotes">
                <hr />
                <ol start="7">
                  <li id="fn7">
                    <p><span class="citation"><a href="references.html#ref-cybiantNaturalLanguageProcessing"
                          role="doc-biblioref">Cybiant</a> (<a
                          href="references.html#ref-cybiantNaturalLanguageProcessing"
                          role="doc-biblioref">n.d.</a>)</span><a href="background.html#fnref7"
                        class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn8">
                    <p><span class="citation"><a href="references.html#ref-WhatNaturalLanguage"
                          role="doc-biblioref"><span>“What Is <span>Natural Language Processing</span>?”</span></a> (<a
                          href="references.html#ref-WhatNaturalLanguage" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref8" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn9">
                    <p>Algorithmic bias describes systematic and repeatable errors in a computer system that create
                      unfair outcomes, such as privileging one arbitrary group of users over others. <span
                        class="citation"><a href="references.html#ref-AlgorithmicBias2020"
                          role="doc-biblioref"><span>“Algorithmic Bias”</span></a> (<a
                          href="references.html#ref-AlgorithmicBias2020" role="doc-biblioref">2020</a>)</span><a
                        href="background.html#fnref9" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn10">
                    <p><span class="citation"><a href="references.html#ref-VocabularyNaturalLanguage"
                          role="doc-biblioref"><span>“Vocabulary - <span>Natural Language Processing</span> with
                            <span>Machine Learning</span>”</span></a> (<a
                          href="references.html#ref-VocabularyNaturalLanguage" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref10" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn11">
                    <p><span class="citation"><a href="references.html#ref-StemmingLemmatization"
                          role="doc-biblioref"><span>“Stemming and Lemmatization”</span></a> (<a
                          href="references.html#ref-StemmingLemmatization" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref11" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn12">
                    <p><span class="citation"><a href="references.html#ref-StemmingLemmatization"
                          role="doc-biblioref"><span>“Stemming and Lemmatization”</span></a> (<a
                          href="references.html#ref-StemmingLemmatization" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref12" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn13">
                    <p><span class="citation"><a href="references.html#ref-brownleeGentleIntroductionBagofWords2017"
                          role="doc-biblioref">Brownlee</a> (<a
                          href="references.html#ref-brownleeGentleIntroductionBagofWords2017"
                          role="doc-biblioref">2017</a>)</span><a href="background.html#fnref13"
                        class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn14">
                    <p>Tf-idf is used to increase the weight of terms which are specific to a single document (or
                      handful of documents) and decrease the weight for terms used in many documents. <span
                        class="citation"><a href="references.html#ref-AnalyzingTextsText2vec"
                          role="doc-biblioref"><span>“Analyzing <span>Texts</span> with the Text2vec Package”</span></a>
                        (<a href="references.html#ref-AnalyzingTextsText2vec" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref14" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn15">
                    <p><span class="citation"><a href="references.html#ref-DocumenttermMatrix2020"
                          role="doc-biblioref"><span>“Document-Term Matrix”</span></a> (<a
                          href="references.html#ref-DocumenttermMatrix2020" role="doc-biblioref">2020</a>)</span><a
                        href="background.html#fnref15" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn16">
                    <p><span class="citation"><a href="references.html#ref-brownleeWhatDeepLearning2019"
                          role="doc-biblioref">Brownlee</a> (<a href="references.html#ref-brownleeWhatDeepLearning2019"
                          role="doc-biblioref">2019</a>)</span><a href="background.html#fnref16"
                        class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn17">
                    <p><span class="citation"><a href="references.html#ref-WhatPerceptronSimplilearn"
                          role="doc-biblioref"><span>“What Is <span>Perceptron</span> |
                            <span>Simplilearn</span>”</span></a> (<a
                          href="references.html#ref-WhatPerceptronSimplilearn" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref17" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn18">
                    <p><span class="citation"><a href="references.html#ref-reginaoftechNeuralNetworksBasics2019"
                          role="doc-biblioref">ReginaOfTech</a> (<a
                          href="references.html#ref-reginaoftechNeuralNetworksBasics2019"
                          role="doc-biblioref">2019</a>)</span><a href="background.html#fnref18"
                        class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn19">
                    <p><span class="citation"><a href="references.html#ref-PerceptronsMultiLayerPerceptrons"
                          role="doc-biblioref"><span>“Perceptrons &amp; <span>Multi</span>-<span>Layer
                              Perceptrons</span>: The <span>Artificial Neuron</span> -
                            <span>MissingLink</span>”</span></a> (<a
                          href="references.html#ref-PerceptronsMultiLayerPerceptrons"
                          role="doc-biblioref">n.d.</a>)</span><a href="background.html#fnref19"
                        class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn20">
                    <p>“Backpropagation is an algorithm for supervised learning of artificial neural networks using
                      gradient descent. Given an artificial neural network and an error function, the method calculates
                      the gradient of the error function with respect to the neural network’s weights.” <span
                        class="citation"><a href="references.html#ref-Backpropagation"
                          role="doc-biblioref"><span>“Backpropagation”</span></a> (<a
                          href="references.html#ref-Backpropagation" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref20" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn21">
                    <p><span class="citation"><a href="references.html#ref-ModelSelectionHow"
                          role="doc-biblioref"><span>“Model Selection - <span>How</span> to Choose the Number of Hidden
                            Layers and Nodes in a Feedforward Neural Network?”</span></a> (<a
                          href="references.html#ref-ModelSelectionHow" role="doc-biblioref">n.d.</a>)</span><a
                        href="background.html#fnref21" class="footnote-back">↩︎</a></p>
                  </li>
                  <li id="fn22">
                    <p><span class="citation"><a href="references.html#ref-brownleeCrashCourseRecurrent2016"
                          role="doc-biblioref">Brownlee</a> (<a
                          href="references.html#ref-brownleeCrashCourseRecurrent2016"
                          role="doc-biblioref">2016</a>)</span><a href="background.html#fnref22"
                        class="footnote-back">↩︎</a></p>
                  </li>
                </ol>
              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="data.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/lunr.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
  <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "sharing": {
          "github": false,
          "facebook": true,
          "twitter": true,
          "linkedin": false,
          "weibo": false,
          "instapaper": false,
          "vk": false,
          "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
        },
        "fontsettings": {
          "theme": "white",
          "family": "sans",
          "size": 2
        },
        "edit": {
          "link": null,
          "text": null
        },
        "history": {
          "link": null,
          "text": null
        },
        "view": {
          "link": null,
          "text": null
        },
        "download": null,
        "toc": {
          "collapse": "subsection"
        }
      });
    });
  </script>

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>