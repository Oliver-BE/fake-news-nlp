
@misc{AlgorithmicBias2020,
  title = {Algorithmic Bias},
  year = {2020},
  month = oct,
  abstract = {Algorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. Bias can emerge due to many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. Algorithmic bias is found across platforms, including but not limited to search engine results and social media platforms, and can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect "systematic and unfair" discrimination. This bias has only recently been addressed in legal frameworks, such as the 2018 European Union's General Data Protection Regulation. As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design. Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias stem from the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single "algorithm" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.},
  annotation = {Page Version ID: 984025862},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/UI7I4H69/index.html},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Algorithmic\_bias},
  journal = {Wikipedia},
  language = {en}
}

@misc{AnalyzingTextsText2vec,
  title = {Analyzing {{Texts}} with the Text2vec Package},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/N4VFHH55/text-vectorization.html},
  howpublished = {https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html\#tf-idf}
}

@misc{asrBigDataQuality2019,
  title = {Big {{Data}} and Quality Data for Fake News and Misinformation Detection:},
  shorttitle = {Big {{Data}} and Quality Data for Fake News and Misinformation Detection},
  author = {Asr, Fatemeh Torabi and Taboada, Maite},
  year = {2019},
  month = may,
  publisher = {{SAGE PublicationsSage UK: London, England}},
  abstract = {Fake news has become an important topic of research in a variety of disciplines including linguistics and computer science. In this paper, we explain how the pr...},
  copyright = {\textcopyright{} The Author(s) 2019},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/LQXNC2N4/Asr and Taboada - 2019 - Big Data and quality data for fake news and misinf.pdf;/Users/oliverbaldwinedwards/Zotero/storage/7XQTVCEJ/2053951719843310.html},
  howpublished = {https://journals.sagepub.com/doi/10.1177/2053951719843310},
  journal = {Big Data \& Society},
  language = {en}
}

@misc{Backpropagation,
  title = {Backpropagation},
  abstract = {Backpropagation, short for \&quot;backward propagation of errors,\&quot; is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network\&\#39;s weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks. The \&quot;backwards\&quot; part of the name stems from the fact that calculation \ldots},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/KJIMY7W9/backpropagation.html},
  howpublished = {https://brilliant.org/wiki/backpropagation/},
  language = {en-us}
}

@misc{BlackBox,
  title = {Black {{Box}}},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/ZPTQMLWM/fulltext.html},
  howpublished = {https://www.thelancet.com/journals/lanres/article/PIIS2213-2600(18)30425-9/fulltext}
}

@misc{brownleeCrashCourseRecurrent2016,
  title = {Crash {{Course}} in {{Recurrent Neural Networks}} for {{Deep Learning}}},
  author = {Brownlee, Jason},
  year = {2016},
  month = jul,
  abstract = {There is another type of neural network that is dominating difficult machine learning problems that involve sequences of inputs called recurrent neural networks. Recurrent neural networks have connections that have loops, adding feedback and memory to the networks over time. This memory allows this type of network to learn and generalize across sequences of inputs [\ldots ]},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/MRRHL8X7/crash-course-recurrent-neural-networks-deep-learning.html},
  howpublished = {https://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/},
  journal = {Machine Learning Mastery},
  language = {en-US}
}

@misc{brownleeGentleIntroductionBagofWords2017,
  title = {A {{Gentle Introduction}} to the {{Bag}}-of-{{Words Model}}},
  author = {Brownlee, Jason},
  year = {2017},
  month = oct,
  abstract = {The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification. In this tutorial, you will discover the bag-of-words model for feature extraction in natural language [\ldots ]},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/TRFZ2TV6/gentle-introduction-bag-words-model.html},
  howpublished = {https://machinelearningmastery.com/gentle-introduction-bag-words-model/},
  journal = {Machine Learning Mastery},
  language = {en-US}
}

@misc{brownleeWhatDeepLearning2019,
  title = {What Is {{Deep Learning}}?},
  author = {Brownlee, Jason},
  year = {2019},
  month = aug,
  abstract = {Deep Learning | Interested in learning more about deep learning and artificial neural networks? Discover exactly what deep learning is by hearing from a range of experts and leaders in the field.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/23Z7LUWA/what-is-deep-learning.html},
  howpublished = {https://machinelearningmastery.com/what-is-deep-learning/},
  journal = {Machine Learning Mastery},
  language = {en-US}
}

@misc{CrossvalidationStatistics2020,
  title = {Cross-Validation (Statistics)},
  year = {2020},
  month = nov,
  abstract = {Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem). One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance. In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.},
  annotation = {Page Version ID: 991134941},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/FJM8JQW7/index.html},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Cross-validation\_(statistics)\&oldid=991134941},
  journal = {Wikipedia},
  language = {en}
}

@misc{cybiantNaturalLanguageProcessing,
  title = {Natural {{Language Processing}}},
  author = {{Cybiant}},
  abstract = {Natural Language Processing can be described as the technology used to aid computers to understand human language.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/9JLCCJUA/natural-language-processing.html},
  howpublished = {https://www.cybiant.com/resources/natural-language-processing/},
  journal = {Cybiant},
  language = {en-US}
}

@misc{desaiResearchGuidesFake,
  title = {Research {{Guides}}: {{Fake News}}, {{Lies}} and {{Propaganda}}: {{How}} to {{Sort Fact}} from {{Fiction}}: {{What}} Is {{Fake News}}?},
  shorttitle = {Research {{Guides}}},
  author = {Desai, Shevon},
  abstract = {Research Guides:},
  copyright = {Copyright University of Michigan Library 2020},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/LS8QNBBR/fakenews.html},
  howpublished = {https://guides.lib.umich.edu/c.php?g=637508\&p=4462356},
  language = {en}
}

@misc{DocumenttermMatrix2020,
  title = {Document-Term Matrix},
  year = {2020},
  month = oct,
  abstract = {A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take. One such scheme is tf-idf. They are useful in the field of natural language processing.},
  annotation = {Page Version ID: 986001463},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/4WWE7QSC/index.html},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Document-term\_matrix\&oldid=986001463},
  journal = {Wikipedia},
  language = {en}
}

@misc{HowBuildNeural,
  title = {How to {{Build}} a {{Neural Network With Keras Using}} the {{IMDB Dataset}}},
  abstract = {Using the IMDB dataset, an AI expert walks through how to build a neural network with Keras.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/Q7HIUAGJ/how-build-neural-network-keras.html},
  howpublished = {https://builtin.com/data-science/how-build-neural-network-keras},
  journal = {Built In},
  language = {en}
}

@misc{LongShorttermMemory2020,
  title = {Long Short-Term Memory},
  year = {2020},
  month = dec,
  abstract = {Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems). A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications.The advantage of an LSTM cell compared to a common recurrent unit is its cell memory unit. The cell vector has the ability to encapsulate the notion of forgetting part of its previously stored memory, as well as to add part of the new information. To illustrate this, one has to inspect the equations of the cell and the way it processes sequences under the hood.},
  annotation = {Page Version ID: 991684445},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/HHITFTIA/index.html},
  howpublished = {https://en.wikipedia.org/w/index.php?title=Long\_short-term\_memory\&oldid=991684445},
  journal = {Wikipedia},
  language = {en}
}

@misc{mitchellManyAmericansSay2019,
  title = {Many {{Americans Say Made}}-{{Up News Is}} a {{Critical Problem That Needs To Be Fixed}}},
  author = {Mitchell, Amy and Gottfried, Jeffrey and Stocking, Galen and Walker, Mason and Fedeli, Sophia},
  year = {2019},
  month = jun,
  abstract = {Politicians viewed as major creators of it, but journalists seen as the ones who should fix it},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/GJZULWNL/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-fixed.html},
  howpublished = {https://www.journalism.org/2019/06/05/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-fixed/},
  journal = {Pew Research Center's Journalism Project},
  language = {en-US}
}

@misc{ModelSelectionHow,
  title = {Model Selection - {{How}} to Choose the Number of Hidden Layers and Nodes in a Feedforward Neural Network?},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/W5NQBAKU/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw.html},
  howpublished = {https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw},
  journal = {Cross Validated}
}

@misc{nagpalL1L2Regularization2017,
  title = {L1 and {{L2 Regularization Methods}}},
  author = {Nagpal, Anuja},
  year = {2017},
  month = oct,
  abstract = {Machine Learning},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/T57HJHXZ/l1-and-l2-regularization-methods-ce25e7fc831c.html},
  howpublished = {https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c},
  journal = {Medium},
  language = {en}
}

@misc{PerceptronsMultiLayerPerceptrons,
  title = {Perceptrons \& {{Multi}}-{{Layer Perceptrons}}: The {{Artificial Neuron}} - {{MissingLink}}},
  shorttitle = {Perceptrons \& {{Multi}}-{{Layer Perceptrons}}},
  abstract = {Learn about the artificial neuron at the core of deep learning\textemdash how a perceptron works, and how they are used in real neural network projects.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/5A6Y5GDX/perceptrons-and-multi-layer-perceptrons-the-artificial-neuron-at-the-core-of-deep-learning.html},
  howpublished = {https://missinglink.ai/guides/neural-network-concepts/perceptrons-and-multi-layer-perceptrons-the-artificial-neuron-at-the-core-of-deep-learning/},
  journal = {MissingLink.ai},
  language = {en-US}
}

@misc{PolitiFact,
  title = {{{PolitiFact}}},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/QJ437ZJ7/www.politifact.com.html},
  howpublished = {https://www.politifact.com/}
}

@misc{reginaoftechNeuralNetworksBasics2019,
  title = {Neural {{Networks}}: {{Basics}}},
  shorttitle = {Neural {{Networks}}},
  author = {ReginaOfTech},
  year = {2019},
  month = nov,
  abstract = {Understanding the basics of neural networks will be beneficial by helping you stay engaged in a conversation around this topic.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/9IMUHWY4/neural-networks-basics-29cc093b82be.html},
  howpublished = {https://towardsdatascience.com/neural-networks-basics-29cc093b82be},
  journal = {Medium},
  language = {en}
}

@misc{shearFactCheckingFinalPresidential2020,
  title = {Fact-{{Checking}} the {{Final Presidential Debate}}},
  author = {Shear, Michael D.},
  year = {2020},
  month = oct,
  abstract = {A team of New York Times journalists fact-checked the debate, providing context and analysis.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/YCDAV7V3/fact-check-debate-trump-biden.html},
  howpublished = {https://www.nytimes.com/live/2020/10/22/us/fact-check-debate-trump-biden},
  journal = {The New York Times},
  language = {en-US}
}

@misc{Snopes,
  title = {Snopes},
  abstract = {The definitive Internet reference source for urban legends, folklore, myths, rumors, and misinformation.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/I36LILT3/www.snopes.com.html},
  howpublished = {https://www.snopes.com/},
  journal = {Snopes.com},
  language = {en-US}
}

@misc{StemmingLemmatization,
  title = {Stemming and Lemmatization},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/SP5N5NQ3/stemming-and-lemmatization-1.html},
  howpublished = {https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html}
}

@misc{taboadaFakeNewsDetection,
  title = {Fake {{News Detection}}},
  author = {Taboada, Maite and Torabi Asr, Fatemeh},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/PZFVLWI9/fakenews.research.sfu.ca.html},
  howpublished = {http://fakenews.research.sfu.ca/},
  journal = {Discourse Processing Lab}
}

@misc{TypesettingNeuralNetwork,
  title = {Typesetting Neural Network Diagrams with {{TeX}} | by {{Darren Reading}} | {{Momenton}} | {{Medium}}},
  howpublished = {https://medium.com/momenton/typesetting-neural-network-diagrams-with-tex-4920b6b9fc19}
}

@misc{VocabularyNaturalLanguage,
  title = {Vocabulary - {{Natural Language Processing}} with {{Machine Learning}}},
  abstract = {Become accustomed to the meaning of "vocabulary" for NLP tasks.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/G7EXEKXU/N0Wr9zwpEmv.html},
  howpublished = {https://www.educative.io/courses/natural-language-processing-ml/N0Wr9zwpEmv},
  journal = {Educative: Interactive Courses for Software Developers},
  language = {en}
}

@misc{WhatNaturalLanguage,
  title = {What Is {{Natural Language Processing}}?},
  abstract = {Natural language processing makes it possible for humans to talk to machines. Find out how our devices understand language and how to apply this technology.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/98ZXPS9R/what-is-natural-language-processing-nlp.html},
  howpublished = {https://www.sas.com/en\_us/insights/analytics/what-is-natural-language-processing-nlp.html},
  language = {en}
}

@misc{WhatPerceptronSimplilearn,
  title = {What Is {{Perceptron}} | {{Simplilearn}}},
  abstract = {Study the Perceptron Tutorial to get the complete overview of Perceptron and how to implement logic gates with Perceptron. Learn about Sigmoid, ReLU, Softmax, and Hyperbolic Tangent Activation Functions in Neural Networks.},
  file = {/Users/oliverbaldwinedwards/Zotero/storage/6S4W9Y6F/what-is-perceptron-tutorial.html},
  howpublished = {https://www.simplilearn.com/what-is-perceptron-tutorial},
  journal = {Simplilearn.com},
  language = {en-US}
}


