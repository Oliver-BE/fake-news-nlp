---
title: "Untitled"
author: "Oliver"
date: "12/5/2020"
output: html_document
---

```{r setup, include=FALSE}
library(rpart)
library(tree)
library(ISLR)
library(yardstick)
library(partykit)
library(mosaic)   # Load additional packages here 
library(dplyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(yardstick)
library(rpart)
library(text2vec)
library(data.table)
library(e1071)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
```

READ IN DATA
```{r}
### DATAFRAME ########################################################################
politifact_df_raw <- read_csv("../data-raw/politifact_phase2_clean_2018_7_3.csv")  
 
# clean up data
politifact_df_cleaned <- politifact_df_raw %>% 
  # get rid of duplicate URLs 
  distinct(politifact_url_phase1, .keep_all = TRUE) %>%  
  filter(fact_tag_phase1 %in% c("True", "Pants on Fire!")) %>% 
  # change desired targets to (categorical) numbers #### changed to numeric for keras 
  mutate(fact_tag_phase1 = as.numeric(ifelse(fact_tag_phase1 == "True", 1, 0))) %>% 
  select(politifact_url_phase1, article_claim_phase1, original_article_text_phase2, fact_tag_phase1)

# add an id 
politifact_df_cleaned$id <- seq.int(nrow(politifact_df_cleaned))
politifact_df_cleaned <- politifact_df_cleaned %>% 
  select(id, article_claim_phase1, original_article_text_phase2, fact_tag_phase1)

glimpse(politifact_df_cleaned)  


# refactor df for corpus 
politifact_corpus_df <- politifact_df_cleaned %>% 
  rename(doc_id = id,
         original_text = article_claim_phase1) %>%  
  mutate(text = lemmatize_strings(removePunctuation(original_text))) %>%   
  select(doc_id, text, fact_tag_phase1)

politifact_corpus_df_text_final <-  politifact_corpus_df %>%  
  mutate(text = stripWhitespace(
    removeWords(
      tolower(
        removeNumbers(text)
        ), stopwords("en")
      ) 
    )
  )
```

TRAIN/TEST SPLIT
```{r}
set.seed(2) 

# 80%: training data, 20%: test data
random <- sample(1:nrow(politifact_corpus_df_text_final), 0.8 * nrow(politifact_corpus_df_text_final)) 
train  <- politifact_corpus_df_text_final[random, ]
test  <- politifact_corpus_df_text_final[-random, ]
```

```{r}
# TRAINING
tokenizer_train <- word_tokenizer(train$text)
train_tokenized <-  itoken(tokenizer_train, ids = train$doc_id, progressbar = FALSE)
   
vocab <-  create_vocabulary(train_tokenized, ngram = c(1L, 3L))
vocab <-  prune_vocabulary(vocab, term_count_min = 5)
 
vectorizer <- vocab_vectorizer(vocab) 
dtm_news_train <- create_dtm(train_tokenized, vectorizer) 

tfidf <- TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_news_train <-  fit_transform(dtm_news_train, tfidf)
###############################################################
# TESTING
tokenizer_test <- word_tokenizer(test$text)
test_tokenized <- itoken(tokenizer_test, ids = test$doc_id, progressbar = FALSE)

# (vocab already taken care of so go right to dtm)
dtm_news_test <- create_dtm(test_tokenized, vectorizer)  
dtm_news_test <- transform(dtm_news_test, tfidf)
```

```{r}
# dtm back into df
final_train_df <- as.data.frame(as.matrix(dtm_news_train))
final_train_df$targetValues <- as.factor(train$'fact_tag_phase1')
glimpse(final_train_df)

final_test_df <- as.data.frame(as.matrix(dtm_news_test))
final_test_df$targetValues <- as.factor(test$'fact_tag_phase1')
glimpse(final_test_df)
```

```{r}
x_train <- final_train_df %>% 
  select(-targetValues)

x_test <- final_test_df %>% 
  select(-targetValues)

y_train <- final_train_df %>% 
  select(targetValues)

y_test <- final_test_df %>% 
  select(targetValues)
```

SVM
```{r}
svm_model <- e1071::svm(x = x_train, y = y_train$targetValues, cost = 1,
                        type = "C-classification", kernel = "sigmoid")
 
pred <- predict(svm_model, x_test)

caret::confusionMatrix(pred, y_test$targetValues)

# 71.5% accuracy
```

 

