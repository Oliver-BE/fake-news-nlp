---
title: "Scratch Work"
author: "Oliver Baldwin Edwards"
date: "11/6/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(mosaic)
library(mdsr)
library(knitr)
library(kableExtra)
# MLP
library(RSNNS)
# stemming
library(tm)
library(qdap)
# unnest tokens
library(tidytext)

# lemma
library(textstem)

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

# Questions
- Does it make sense to give more weight to articles that are sourced from multiple different websites (such as entries 1-3)?
- **I think it makes sense to just keep one entry for each**

- PoltiFact claims such as "False, True, etc." are all based on the claim that the article makes, and not the article itself. For example, if an article is writing about a claim that's false, and casts doubt on it, then the article text itself is not necessarily false (even though the claim is). How do I deal with this?

- examine how MLP neural networks work and figure out if less text is better

- how do i add footnotes to my references so they all line up
# Links

for exposition on fake news see: https://journals.sagepub.com/doi/10.1177/2053951719843310
"Another form of automatic checking involves assessing the language of the story itself, i.e., finding cues in the language of the story that point to exaggerated claims, overly emotional language or a style that is uncommon in mainstream news sources. This is, in essence, a text classification problem, one commonly addressed by computational linguists using NLP tools. Potthast et al. (2018) describe this type of classification as style-based fake news detection, as opposed to context-based (exploring the social network of the posts and the posters) or knowledge-based detection (fact-checking)."

# The Data

```{r message=FALSE, warning=FALSE}
setwd("~/Dropbox (Amherst College)/Mirror/Amherst_2021/7_Fall_2020/STAT495/final_project/STAT495F20-project-Baldwin-Edwards/scratch-work")

# df_emergent <- read_csv("../data-raw/emergent_phase2_clean_2018_7_2.csv")
df_politifact <- read_csv("../data-raw/politifact_phase2_clean_2018_7_3.csv")
df_snopes <- read_csv("../data-raw/snopes_phase2_clean_2018_7_3.csv")
```

## Initial Data 

```{r warning=FALSE}
# look at classification counts
count(df_politifact, fact_tag_phase1)
```

Description of different ratings from PolitiFact can be found here: https://www.politifact.com/article/2011/feb/21/principles-truth-o-meter/

- TRUE – The statement is accurate and there’s nothing significant missing.
- MOSTLY TRUE – The statement is accurate but needs clarification or additional information.
- HALF TRUE – The statement is partially accurate but leaves out important details or takes things out of context.
- MOSTLY FALSE – The statement contains an element of truth but ignores critical facts that would give a different impression.
- FALSE – The statement is not accurate.
- PANTS ON FIRE – The statement is not accurate and makes a ridiculous claim.

Note that The Flip-O-Meter (Full Flop, Half Flip, and No Flip) describes whether an elected official has been consistent on an issue.

- No Flip – No significant change in position.
- Half Flip – A partial change in position.
- Full Flop – A complete change in position.

```{r}
count(df_snopes, fact_rating_phase1)
```

Note that all above NA values are actually "Mostly True values". (See the 6213 parsing failures when the file is read in).

## Initial Data Cleaning

```{r}
# clean up data
df_politifact_uniques <- df_politifact %>% 
  # get rid of duplicate URLs 
  distinct(politifact_url_phase1, .keep_all = TRUE) %>% 
  # get rid of flip-o-meter ratings
  filter(!(fact_tag_phase1 %in% c("Full Flop", "Half Flip", "No Flip")))
```

```{r}
count(df_politifact_uniques, fact_tag_phase1)
glimpse(df_politifact_uniques)
```


## Stemming/Lemmatization

```{r}
stemDocument("testing")
stemDocument(c("computational", "computers", "computation"))
```

```{r}
x <- c("completed","complete","completion","teach","taught")
tm <- Corpus(VectorSource(x))
tm <- tm_map(tm, stemDocument)
inspect(tm)
dictCorpus <- tm

tm <- tm_map(tm, stemCompletion, dictionary=dictCorpus)
```

```{r}
# can also use removeNumbers and stripWhiteSpace and replace_abbreviation() and bracketX()

trump <- removePunctuation("'Trump approval rating better than Obama and Reagan at same point in their presidencies.'")
frequent_terms <- freq_terms(trump, top = 4)
frequent_terms

unpacked_trump <- unlist(strsplit(trump, split = " "))
stem_trump <- stemDocument(unpacked_trump)
stem_trump
```


## Stop words 

```{r}
# use removeWords to remove stop words
all_stops <- c("word1", "word2", stopwords("en"))
trump_stop <- removeWords(trump, all_stops)
trump_stop <- stripWhitespace(trump_stop)
trump_stop
```

```{r}
# working example
trump <- "'Trump approval rating better than Obama and Reagan at same point in their presidencies.'" 
trump_corpus <-  VCorpus(VectorSource(trump))
content(trump_corpus[[1]])

trump_corpus <- tm_map(trump_corpus, removePunctuation)
all_stops <- c("word1", "word2", stopwords("en"))
trump_corpus <- tm_map(trump_corpus, removeWords, all_stops)
trump_corpus <- tm_map(trump_corpus, stripWhitespace)
content(trump_corpus[[1]])

# trump_corpus <- tm_map(trump_corpus, stemDocument)
trump_corpus <- tm_map(trump_corpus,lemmatize_strings)
# trump_corpus <- tm_map(trump_corpus, PlainTextDocument)
content(trump_corpus[[1]])
```


```{r}
# unnest tokens
test_df <- unnest_tokens(df_politifact_uniques, word, article_claim_phase1)
glimpse(test_df)

# lemma
test_tokens <-  unlist(strsplit(removePunctuation("'Trump approval rating better than Obama and Reagan at same point in their presidencies.'"), split = " "))
lemmatize_words(test_tokens)
```


