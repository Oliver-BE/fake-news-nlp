---
title: "Untitled"
author: "Oliver"
date: "12/5/2020"
output: html_document
---

```{r setup, include=FALSE}
library(rpart)
library(tree)
library(ISLR)
library(yardstick)
library(partykit)
library(mosaic)   # Load additional packages here 
library(dplyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(yardstick)
library(rpart)
library(text2vec)
library(data.table)
library(e1071)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
```

READ IN DATA
```{r}
### DATAFRAME ########################################################################
politifact_df_raw <- read_csv("../data-raw/politifact_phase2_clean_2018_7_3.csv")  
 
# clean up data
politifact_df_cleaned <- politifact_df_raw %>% 
  # get rid of duplicate URLs 
  distinct(politifact_url_phase1, .keep_all = TRUE) %>%  
  filter(fact_tag_phase1 %in% c("True", "Pants on Fire!")) %>% 
  # change desired targets to (categorical) numbers #### changed to numeric for keras 
  mutate(fact_tag_phase1 = as.numeric(ifelse(fact_tag_phase1 == "True", 1, 0))) %>% 
  select(politifact_url_phase1, article_claim_phase1, original_article_text_phase2, fact_tag_phase1)

# add an id 
politifact_df_cleaned$id <- seq.int(nrow(politifact_df_cleaned))
politifact_df_cleaned <- politifact_df_cleaned %>% 
  select(id, article_claim_phase1, original_article_text_phase2, fact_tag_phase1)

glimpse(politifact_df_cleaned)  


# refactor df for corpus 
politifact_corpus_df <- politifact_df_cleaned %>% 
  rename(doc_id = id,
         original_text = article_claim_phase1) %>%  
  mutate(text = lemmatize_strings(removePunctuation(original_text))) %>%   
  select(doc_id, text, fact_tag_phase1)

politifact_corpus_df_text_final <-  politifact_corpus_df %>%  
  mutate(text = stripWhitespace(
    removeWords(
      tolower(
        removeNumbers(text)
        ), stopwords("en")
      ) 
    )
  )
```


```{r}
set.seed(2)

# 80%: training data, 20%: test data
random <- sample(1:nrow(politifact_corpus_df_text_final), 0.8 * nrow(politifact_corpus_df_text_final)) 
train  <- politifact_corpus_df_text_final[random, ]
test  <- politifact_corpus_df_text_final[-random, ]
```

```{r}
# TRAINING
tokenizer_train <- word_tokenizer(train$text)
train_tokenized <-  itoken(tokenizer_train, ids = train$doc_id, progressbar = FALSE)
   
vocab <-  create_vocabulary(train_tokenized, ngram = c(1L, 3L))
vocab <-  prune_vocabulary(vocab, term_count_min = 5)
 
vectorizer <- vocab_vectorizer(vocab) 
dtm_news_train <- create_dtm(train_tokenized, vectorizer) 

tfidf <- TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_news_train <-  fit_transform(dtm_news_train, tfidf)
###############################################################
# TESTING
tokenizer_test <- word_tokenizer(test$text)
test_tokenized <- itoken(tokenizer_test, ids = test$doc_id, progressbar = FALSE)

# (vocab already taken care of so go right to dtm)
dtm_news_test <- create_dtm(test_tokenized, vectorizer)  
dtm_news_test <- transform(dtm_news_test, tfidf)
```

```{r}
# dtm back into df
final_train_df <- as.data.frame(as.matrix(dtm_news_train))
final_train_df$targetValues <- as.numeric(train$'fact_tag_phase1')
glimpse(final_train_df)

final_test_df <- as.data.frame(as.matrix(dtm_news_test))
final_test_df$targetValues <- as.numeric(test$'fact_tag_phase1')
glimpse(final_test_df)
```

KERAS NEEDS PREDICTOR INPUTS AS A MATRIX, NOT DF
```{r}
x_train <- final_train_df %>% 
  select(-targetValues)
x_train <- as.matrix(x_train)

x_test <- final_test_df %>% 
  select(-targetValues)
x_test <- as.matrix(x_test)

y_train <- final_train_df %>% 
  select(targetValues)
y_train <- y_train$targetValues

y_test <- final_test_df %>% 
  select(targetValues)
y_test <- y_test$targetValues
```

Test for balance of targets in train/test split
```{r}
#Proportion for train targets
prop.table(table(y_train))

#Proportion for test targets
prop.table(table(y_test))
```


### MLP ########################################################################

```{r}
# take 25% of training (20% of all data since training is 80%) and turn it into validation 
# num_validation_rows <- ceiling(nrow(x_train) * 0.25)
# val_indices <- 1:num_validation_rows
# 
# x_validation <- x_train[val_indices,]
# partial_x_train <- x_train[-val_indices,]
# 
# y_validation <- politifact_train_targets[val_indices]
# partial_y_train <- politifact_train_targets[-val_indices] 
```

KERAS NEEDS TARGETS AS NUMERIC
```{r}
num_predictors <- ncol(x_train)
# input layer implicitly added with input_shape
model <- keras_model_sequential() %>% 
  layer_dense(units = 8, activation = "relu", 
              input_shape = c(num_predictors)) %>%   
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# make sure we use the keras fit function
history <- model %>% keras::fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 1,
  validation_split = 0.25
) 
plot(history) 
```

batch size:  It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. (https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu)
```{r}
# for choosing num layers: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw
model <- keras_model_sequential() %>% 
  layer_dense(units = 8, activation = "relu", input_shape = c(num_predictors)) %>%  
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# for batch size info: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/
model %>% keras::fit(x_train, y_train, epochs = 5, batch_size = 1)
results <- model %>% evaluate(x_test, y_test)

classes <- model %>% predict_classes(x_test, batch_size = 1)

# Confusion matrix
table(y_test, classes)
# on avg, 70%
```

